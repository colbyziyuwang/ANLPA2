{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to update parent_folder and colab_base as you see fit\n",
    "\n",
    "Download Ollama first to access llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global paths for both local (Mac) and Google Colab\n",
    "PARENT_FOLDER = \"/Users/colbywang/Google Drive/ÊàëÁöÑ‰∫ëÁ´ØÁ°¨Áõò/Advanced NLP/Assignments/data files/organized/\"\n",
    "COLAB_BASE = \"/content/gdrive/MyDrive/Assignments/Advanced NLP/Assignments/data files/organized/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load FinBERT model and tokenizer\n",
    "model_name = \"yiyanghkust/finbert-pretrain\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Optimize for GPU and efficiency\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval().half()  # Convert model to eval mode and use float16\n",
    "\n",
    "def get_average_embedding(sentences, batch_size=16):\n",
    "    \"\"\"Compute and average sentence embeddings using FinBERT with batch processing.\"\"\"\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}  # Move to GPU\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Extract [CLS] token embeddings\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # Move back to CPU\n",
    "        all_embeddings.append(cls_embeddings)\n",
    "\n",
    "    # Stack all batches and compute the mean embedding\n",
    "    avg_embedding = np.mean(np.vstack(all_embeddings), axis=0)\n",
    "\n",
    "    return avg_embedding # shape (768,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings\n",
    ")\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.packs.sentence_window_retriever import SentenceWindowRetrieverPack as SentenceWindowRetriever\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "import torch\n",
    "\n",
    "# ‚úÖ Load the LLM Model (Check if GPU is Available for Faster Inference)\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2\",\n",
    "    context_window=4096,\n",
    "    request_timeout=600.0,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "embedding_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    device=device  # Move embedding model to GPU if possible\n",
    ")\n",
    "\n",
    "# ‚úÖ Configure Settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_model\n",
    "\n",
    "# ‚úÖ Create Node Parser with Sentence Window (Used in Function)\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=2,  # Use a slightly larger window to improve context\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\"\n",
    ")\n",
    "\n",
    "def run_rag_pipeline(file_path, query_text, similarity_top_k=3):\n",
    "    \"\"\"\n",
    "    Optimized RAG pipeline for querying a document.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the 10-K or DEF 14A file.\n",
    "        query_text (str): The query to ask the LLM.\n",
    "        similarity_top_k (int): Number of top similar documents to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        str: The retrieved response from the document.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ‚úÖ Load document (Lazy Loading for Large Files)\n",
    "        docs = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "\n",
    "        if not docs:\n",
    "            return \"‚ö†Ô∏è No content found in the document!\"\n",
    "\n",
    "        # ‚úÖ Process nodes from document\n",
    "        nodes = node_parser.get_nodes_from_documents(docs)\n",
    "\n",
    "        if not nodes:\n",
    "            return \"‚ö†Ô∏è No nodes extracted from the document!\"\n",
    "\n",
    "        # ‚úÖ Create Vector Store Index (Avoid Duplicate Indexing)\n",
    "        index = VectorStoreIndex.from_documents(docs)  # Directly from documents (faster)\n",
    "\n",
    "        # ‚úÖ Create Retriever (Dynamically Adjust `top_k`)\n",
    "        retriever = index.as_retriever(similarity_top_k=similarity_top_k)\n",
    "\n",
    "        # ‚úÖ Create Query Engine\n",
    "        query_engine = RetrieverQueryEngine(retriever=retriever)\n",
    "\n",
    "        # ‚úÖ Run the query\n",
    "        response = query_engine.query(query_text)\n",
    "\n",
    "        return response.response if response else \"‚ö†Ô∏è No relevant information found!\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error processing file: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_file_path(colab_path):\n",
    "    \"\"\"\n",
    "    Converts a file path from Google Drive (Colab) to a local path.\n",
    "\n",
    "    Args:\n",
    "        colab_path (str): The file path from Google Drive in Colab.\n",
    "\n",
    "    Returns:\n",
    "        str: The equivalent local path.\n",
    "    \"\"\"\n",
    "    local_path = colab_path.replace(COLAB_BASE, PARENT_FOLDER, 1)\n",
    "    return local_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 347 files and testing on 108 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Stock Data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 347/347 [00:06<00:00, 55.64it/s]\n",
      "Processing Stock Data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 108/108 [00:00<00:00, 114505.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 131\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m    130\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 131\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    134\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Desktop/ANLPA2/rag/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ANLPA2/rag/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ANLPA2/rag/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ‚úÖ Define Parent Folder for Stock Data\n",
    "PARENT_FOLDER = \"/Users/colbywang/Google Drive/ÊàëÁöÑ‰∫ëÁ´ØÁ°¨Áõò/Advanced NLP/Assignments/data files/organized/stock-data/\"\n",
    "\n",
    "# ‚úÖ Get list of all CSV files in stock data folder\n",
    "csv_files = [os.path.join(PARENT_FOLDER, file) for file in os.listdir(PARENT_FOLDER) if file.endswith(\".csv\")]\n",
    "\n",
    "# ‚úÖ Train/Test Split (80% train, 20% test)\n",
    "train_files = csv_files[:int(0.7 * len(csv_files))]\n",
    "test_files = csv_files[int(0.7 * len(csv_files))]\n",
    "print(f\"Training on {len(train_files)} files and testing on {len(test_files)} files.\")\n",
    "\n",
    "# ‚úÖ Define Function to Load & Preprocess Stock Data\n",
    "def load_and_preprocess_data(stock_file):\n",
    "    # Check if file is empty\n",
    "    if os.stat(stock_file).st_size == 0:\n",
    "        return None, None\n",
    "\n",
    "    df = pd.read_csv(stock_file, parse_dates=[\"Date\"])\n",
    "    df.set_index(\"Date\", inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # ‚úÖ Select Features\n",
    "    features = df[[\"Close\", \"High\", \"Low\", \"Open\", \"Volume\", \"Percentage Change\", \"CPI\", \"Inflation\",\n",
    "                   \"3 Mo\", \"6 Mo\", \"1 Yr\", \"2 Yr\", \"3 Yr\", \"5 Yr\", \"7 Yr\", \"10 Yr\", \"20 Yr\"]]\n",
    "\n",
    "    # ‚úÖ Shift the Target (Predict next day's percentage change)\n",
    "    df[\"Target\"] = df[\"Percentage Change\"].shift(-1)\n",
    "\n",
    "    # ‚úÖ Drop rows with NaN values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return features, df[\"Target\"]\n",
    "\n",
    "# ‚úÖ Define GRU Model\n",
    "class StockGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(StockGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        output = self.fc(output[:, -1, :])  # Take last timestep output\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "# ‚úÖ Device Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ‚úÖ Prepare Data Function\n",
    "def prepare_data(stock_files, sequence_length=10):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for stock_file in tqdm(stock_files, desc=\"Processing Stock Data\"):\n",
    "        if not os.path.isfile(stock_file):  # ‚úÖ Ensure it's a file\n",
    "            continue  # Skip directories\n",
    "\n",
    "        features, target = load_and_preprocess_data(stock_file)\n",
    "\n",
    "        if features is None or target is None:\n",
    "            continue\n",
    "        features, target = features.values, target.values\n",
    "\n",
    "        for i in range(len(features) - sequence_length):\n",
    "            if i + sequence_length >= len(target):  # Ensure index is within bounds\n",
    "                break\n",
    "            X.append(features[i:i + sequence_length])\n",
    "            y.append(target[i + sequence_length])\n",
    "\n",
    "\n",
    "    # Convert X and y to NumPy arrays with explicit dtype\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_tensor = torch.from_numpy(X).to(device)\n",
    "    y_tensor = torch.from_numpy(y).to(device)\n",
    "\n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# ‚úÖ Prepare Training & Testing Data\n",
    "sequence_length = 10\n",
    "X_train, y_train = prepare_data(train_files, sequence_length)\n",
    "X_test, y_test = prepare_data(test_files, sequence_length)\n",
    "\n",
    "# ‚úÖ Initialize GRU Model\n",
    "input_size = X_train.shape[2]\n",
    "hidden_size = 64\n",
    "gru_model = StockGRU(input_size, hidden_size).to(device)\n",
    "\n",
    "# ‚úÖ Define Loss, Optimizer, and Training Parameters\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(gru_model.parameters(), lr=0.001)\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "training_losses = []  # Store losses\n",
    "\n",
    "# ‚úÖ Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"üîµ Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    total_loss = 0\n",
    "    hidden = gru_model.init_hidden(batch_size)\n",
    "\n",
    "    for i in tqdm(range(0, len(X_train), batch_size), desc=\"training batches\", leave=False):\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "\n",
    "        if len(batch_X) < batch_size:\n",
    "            continue\n",
    "\n",
    "        hidden = hidden.detach()\n",
    "\n",
    "        # Forward Pass\n",
    "        output, hidden = gru_model(batch_X, hidden)\n",
    "        loss = loss_fn(output.squeeze(), batch_y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / (len(X_train) // batch_size)\n",
    "    training_losses.append(avg_loss)  # Store loss\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ‚úÖ Plot & Save Training Loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, num_epochs + 1), training_losses, marker=\"o\", linestyle=\"-\", color=\"blue\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss (MSE)\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.savefig(\"training_loss.png\")\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ Evaluation on Test Set\n",
    "gru_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_hidden = gru_model.init_hidden(X_test.size(0))\n",
    "    y_pred, _ = gru_model(X_test, test_hidden)\n",
    "    y_pred = y_pred.cpu().numpy().flatten()\n",
    "    y_test = y_test.cpu().numpy().flatten()\n",
    "\n",
    "    # ‚úÖ Calculate Performance Metrics\n",
    "    mae = np.mean(np.abs(y_pred - y_test))\n",
    "    mse = np.mean((y_pred - y_test) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    print(\"\\nüîπ **Test Set Performance Metrics** üîπ\")\n",
    "    print(f\"üìâ Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"üìä Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"üìà Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "    # ‚úÖ Select a Random Test Example for Visualization\n",
    "    random_idx = np.random.randint(0, len(y_test) - 20)  # Pick a random starting point\n",
    "    actual_values = y_test[random_idx:random_idx + 20]\n",
    "    predicted_values = y_pred[random_idx:random_idx + 20]\n",
    "\n",
    "    # ‚úÖ Plot & Save Actual vs Predicted\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(actual_values, label=\"Actual\", marker=\"o\", linestyle=\"-\", color=\"green\")\n",
    "    plt.plot(predicted_values, label=\"Predicted\", marker=\"x\", linestyle=\"--\", color=\"red\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Stock Percentage Change\")\n",
    "    plt.title(\"Actual vs Predicted Stock Percentage Change\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.savefig(\"actual_vs_pred.png\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
