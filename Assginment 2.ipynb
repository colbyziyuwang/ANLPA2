{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# GRU Model Definition\n",
    "class GRUStockModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRUStockModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # Take last time step output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to update parent_folder and colab_base as you see fit\n",
    "\n",
    "Download Ollama first to access llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global paths for both local (Mac) and Google Colab\n",
    "PARENT_FOLDER = \"/Users/colbywang/Google Drive/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/Advanced NLP/Assignments/data files/organized/\"\n",
    "COLAB_BASE = \"/content/gdrive/MyDrive/Assignments/Advanced NLP/Assignments/data files/organized/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# âœ… Initialize Embedding Model\n",
    "embedding_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# âœ… Precompute Zero Vector (384-D)\n",
    "ZERO_VECTOR = np.zeros((384,))\n",
    "\n",
    "def retrieve_avg_embedding(file_path):\n",
    "    \"\"\"\n",
    "    Efficiently compute the SEC filing embedding **without RAG**.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): SEC filing path.\n",
    "        top_k (int): Ignored (kept for compatibility).\n",
    "        is_10K (bool): Whether this is a 10-K or DEF 14A filing.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Computed SEC embedding (shape: 384,).\n",
    "    \"\"\"\n",
    "\n",
    "    # âœ… Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        return ZERO_VECTOR\n",
    "\n",
    "    # âœ… Read the SEC filing content\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # âœ… Compute the embedding using Hugging Face\n",
    "    embedding = embedding_model.get_text_embedding(text)\n",
    "\n",
    "    return np.array(embedding)  # Shape: (384,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_file_path(google_drive_path):\n",
    "    \"\"\"\n",
    "    Convert Google Drive file paths to local file paths.\n",
    "    \n",
    "    Args:\n",
    "        google_drive_path (str): File path stored in the CSV (Google Drive format).\n",
    "    \n",
    "    Returns:\n",
    "        str: Converted local file path.\n",
    "    \"\"\"\n",
    "    google_prefix = \"/content/gdrive/MyDrive/Assignments\"  # Incorrect Google Drive path\n",
    "    local_prefix = \"/Users/colbywang/Google Drive/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜\"  # Correct local prefix\n",
    "\n",
    "    if google_drive_path.startswith(google_prefix):\n",
    "        local_path = google_drive_path.replace(google_prefix, local_prefix)\n",
    "        return local_path\n",
    "    \n",
    "    return google_drive_path  # If not found, return as is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 347 stock files, Testing on 150 stock files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading training files:   0%|          | 0/347 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading training files:   2%|â–         | 7/347 [00:03<03:10,  1.79it/s]\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mstat(file)\u001b[38;5;241m.\u001b[39mst_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVolume\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCPI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInflation\u001b[39m\u001b[38;5;124m'\u001b[39m}\u001b[38;5;241m.\u001b[39missubset(df\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip if missing columns\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ANLPA2/rag/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ANLPA2/rag/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Desktop/ANLPA2/rag/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ANLPA2/rag/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/ANLPA2/rag/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to stock data folder\n",
    "PARENT_FOLDER = \"/Users/colbywang/Google Drive/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/Advanced NLP/Assignments/data files/organized/stock-data\"\n",
    "all_files = glob(os.path.join(PARENT_FOLDER, \"*.csv\"))\n",
    "\n",
    "# Hyperparameters\n",
    "sequence_length = 7  # Lookback window\n",
    "N = 1  # Predict trend in next N days\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "threshold = 0.02  # 2% change threshold for \"Stable\"\n",
    "train_ratio = 0.7  # 70% of files for training, 30% for testing\n",
    "\n",
    "# Split stock files into training and testing sets\n",
    "train_files, test_files = train_test_split(all_files, test_size=1-train_ratio, random_state=42)\n",
    "print(f\"Training on {len(train_files)} stock files, Testing on {len(test_files)} stock files\")\n",
    "\n",
    "def create_labels(df):\n",
    "    \"\"\"\n",
    "    Create classification labels for stock price movement.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Stock data\n",
    "        N (int): Number of days in the future to compute price movement\n",
    "        threshold (float): Percentage change threshold to classify Up or Down\n",
    "\n",
    "    Returns:\n",
    "        Series: Labels (0 = Stable, 1 = Up, 2 = Down)\n",
    "    \"\"\"\n",
    "    df['pct change'] = df['Close'].pct_change(N)\n",
    "    df['Close_diff_pct'] = df['pct change'].shift(-N)\n",
    "\n",
    "    # Default to \"Stable\"\n",
    "    df['Label'] = 0  \n",
    "\n",
    "    # Up (if change > threshold)\n",
    "    df.loc[df['Close_diff_pct'] > threshold, 'Label'] = 1  \n",
    "\n",
    "    # Down (if change < -threshold)\n",
    "    df.loc[df['Close_diff_pct'] < -threshold, 'Label'] = 2  \n",
    "\n",
    "    return df['Label']\n",
    "\n",
    "def create_sequences(df, labels, sequence_length, N=5):\n",
    "    \"\"\"\n",
    "    Create sequences for GRU training.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Stock features\n",
    "        labels (Series): Classification labels\n",
    "        sequence_length (int): Length of input sequence\n",
    "        N (int): Lookahead period for labels\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: X (features)\n",
    "        np.ndarray: y (labels)\n",
    "    \"\"\"\n",
    "    df = df[['Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation']]\n",
    "    X, y = [], []\n",
    "\n",
    "    # Ensure labels align with future price movement\n",
    "    for i in range(len(df) - sequence_length - N):\n",
    "        X.append(df.values[i:i + sequence_length])  # Sequence of input features\n",
    "        y.append(labels.iloc[i + sequence_length])  # Label for the next movement\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load and preprocess all training stock data\n",
    "all_X_train, all_y_train = [], []\n",
    "\n",
    "for file in tqdm(train_files, desc=\"Loading training files\"):\n",
    "    # If file is empty skip\n",
    "    if os.stat(file).st_size == 0:\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(file)\n",
    "    if not {'Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation'}.issubset(df.columns):\n",
    "        continue  # Skip if missing columns\n",
    "\n",
    "    df = df[['Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation']]\n",
    "\n",
    "    labels = create_labels(df)\n",
    "    X, y = create_sequences(df, labels, sequence_length)\n",
    "\n",
    "    all_X_train.append(X)\n",
    "    all_y_train.append(y)\n",
    "\n",
    "# Concatenate all training sequences\n",
    "X_train = np.concatenate(all_X_train, axis=0)\n",
    "y_train = np.concatenate(all_y_train, axis=0)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "# Create PyTorch DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize Model\n",
    "input_size = 7  # ['Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation']\n",
    "output_size = 3  # 3 classes: Up, Down, Stable\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GRUStockModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train Model\n",
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        # If batch_X has nan skip\n",
    "        if torch.isnan(batch_X).any():\n",
    "            continue\n",
    "        \n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save Training Loss Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"train_loss.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nâœ… Training complete. Training loss visualization saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved as 'stock_model.pth'.\n"
     ]
    }
   ],
   "source": [
    "# Save Model\n",
    "torch.save(model.state_dict(), \"stock_gru_model.pth\")\n",
    "print(\"âœ… Model saved as 'stock_model.pth'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stock files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [06:29<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ **Overall Test Set Performance Metrics** ðŸ”¹\n",
      "âœ… Accuracy: 0.7716\n",
      "ðŸ“ Precision: 0.6080\n",
      "ðŸ“¡ Recall: 0.7716\n",
      "âš–ï¸ F1-Score: 0.6726\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# âœ… Define Parent Folder for Stock Data\n",
    "PARENT_FOLDER = \"/Users/colbywang/Google Drive/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/Advanced NLP/Assignments/data files/organized/stock-data/\"\n",
    "\n",
    "# âœ… Get list of test files\n",
    "csv_files = [os.path.join(PARENT_FOLDER, file) for file in os.listdir(PARENT_FOLDER) if file.endswith(\".csv\")]\n",
    "test_files = csv_files[int(0.7 * len(csv_files)):]\n",
    "\n",
    "# âœ… Load GRU Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GRUStockModel(input_size=7, hidden_size=64, num_layers=2, output_size=3).to(device)\n",
    "model.load_state_dict(torch.load(\"stock_gru_model.pth\"))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# âœ… Initialize Metrics Storage\n",
    "all_actuals, all_predictions = [], []\n",
    "\n",
    "# âœ… Process Each Test Stock\n",
    "for test_stock in tqdm(test_files, desc=\"stock files\"):\n",
    "    # If file is empty skip\n",
    "    if os.stat(test_stock).st_size == 0:\n",
    "        continue\n",
    "\n",
    "    df_test = pd.read_csv(test_stock)\n",
    "    if not {'Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation'}.issubset(df.columns):\n",
    "        continue  # Skip if missing columns\n",
    "\n",
    "    # Select Relevant Features\n",
    "    df_test = df_test[['Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation']]\n",
    "\n",
    "    # Generate Labels\n",
    "    labels_test = create_labels(df_test)\n",
    "\n",
    "    # Create Sequences\n",
    "    X_test, y_test = create_sequences(df_test, labels_test, sequence_length=7)\n",
    "    X_test, y_test = torch.tensor(X_test, dtype=torch.float32).to(device), torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    # âœ… Run Inference\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X_test)):\n",
    "            X_sample = X_test[i].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "            if torch.isnan(X_sample).any():\n",
    "                continue\n",
    "\n",
    "            y_actual = y_test[i].item()\n",
    "            output = model(X_sample)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            # Store results\n",
    "            all_actuals.append(y_actual)\n",
    "            all_predictions.append(predicted.item())\n",
    "\n",
    "# âœ… Compute Classification Metrics\n",
    "accuracy = accuracy_score(all_actuals, all_predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_actuals, all_predictions, average=\"weighted\", zero_division=0)\n",
    "\n",
    "# âœ… Print Results\n",
    "print(\"\\nðŸ”¹ **Overall Test Set Performance Metrics** ðŸ”¹\")\n",
    "print(f\"âœ… Accuracy: {accuracy:.4f}\")\n",
    "print(f\"ðŸ“ Precision: {precision:.4f}\")\n",
    "print(f\"ðŸ“¡ Recall: {recall:.4f}\")\n",
    "print(f\"âš–ï¸ F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with RAG Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# âœ… Path to stock data folder\n",
    "stock_folder = \"/Users/colbywang/Google Drive/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/Advanced NLP/Assignments/data files/organized/stock-data\"\n",
    "all_files = glob(os.path.join(stock_folder, \"*.csv\"))\n",
    "\n",
    "# âœ… Hyperparameters\n",
    "sequence_length = 7  # Lookback window\n",
    "N = 1  # Predict trend in next N days\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "threshold = 0.02  # 2% change threshold for \"Stable\"\n",
    "train_ratio = 0.7  # 70% of files for training, 30% for testing\n",
    "embedding_dim = 384  # SEC Filing Embedding Size\n",
    "\n",
    "# âœ… Split stock files into training and testing sets\n",
    "train_files, test_files = train_test_split(all_files, test_size=1-train_ratio, random_state=42)\n",
    "print(f\"Training on {len(train_files)} stock files, Testing on {len(test_files)} stock files\")\n",
    "\n",
    "def create_labels(df):\n",
    "    \"\"\"Create classification labels for stock price movement.\"\"\"\n",
    "    df['pct change'] = df['Close'].pct_change(N)\n",
    "    df['Close_diff_pct'] = df['pct change'].shift(-N)\n",
    "    df['Label'] = 0  \n",
    "    df.loc[df['Close_diff_pct'] > threshold, 'Label'] = 1  \n",
    "    df.loc[df['Close_diff_pct'] < -threshold, 'Label'] = 2  \n",
    "    return df['Label']\n",
    "\n",
    "def create_sequences(df, labels, filing_embeddings, sequence_length):\n",
    "    \"\"\"\n",
    "    Create sequences for GRU training including SEC filing embeddings.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Stock features.\n",
    "        labels (Series): Classification labels.\n",
    "        filing_embeddings (np.ndarray): Precomputed embeddings for filings.\n",
    "        sequence_length (int): Length of input sequence.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: X (features including filing embeddings)\n",
    "        np.ndarray: y (labels)\n",
    "    \"\"\"\n",
    "    stock_features = df[['Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation']]\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(len(stock_features) - sequence_length - N):\n",
    "        stock_seq = stock_features.values[i:i + sequence_length]  # (seq_len, 7)\n",
    "        filing_seq = filing_embeddings[i:i + sequence_length]  # (seq_len, 384)\n",
    "\n",
    "        combined_seq = np.hstack((stock_seq, filing_seq))  # (seq_len, 391)\n",
    "        X.append(combined_seq)\n",
    "        y.append(labels.iloc[i + sequence_length])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def load_filing_embeddings(csv_file):\n",
    "    \"\"\"\n",
    "    Load SEC filing embeddings from CSV.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to stock CSV file.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Filing embeddings for each row.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # âœ… Check if embedding column exists\n",
    "    if \"embedding\" not in df.columns:\n",
    "        print(f\"âš ï¸ No embeddings found in {csv_file}, using zero vectors.\")\n",
    "        return np.zeros((len(df), embedding_dim))\n",
    "\n",
    "    # âœ… Convert stored string embeddings to numpy array\n",
    "    return np.array(df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\",\")))\n",
    "\n",
    "# âœ… Load and preprocess all training stock data\n",
    "all_X_train, all_y_train = [], []\n",
    "\n",
    "for file in tqdm(train_files, desc=\"Loading training files\"):\n",
    "    if os.stat(file).st_size == 0:\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(file)\n",
    "    if not {'Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation', 'embedding'}.issubset(df.columns):\n",
    "        continue\n",
    "\n",
    "    df = df[['Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation', 'embedding']]\n",
    "    \n",
    "    labels = create_labels(df)\n",
    "    filing_embeddings = load_filing_embeddings(file)  # Load embeddings from CSV\n",
    "    X, y = create_sequences(df, labels, filing_embeddings, sequence_length)\n",
    "\n",
    "    all_X_train.append(X)\n",
    "    all_y_train.append(y)\n",
    "\n",
    "# âœ… Concatenate all training sequences\n",
    "X_train = np.concatenate(all_X_train, axis=0)  # (total_samples, seq_len, 391)\n",
    "y_train = np.concatenate(all_y_train, axis=0)\n",
    "\n",
    "# âœ… Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "# âœ… Create PyTorch DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# âœ… Initialize Model\n",
    "input_size = 391  # Stock features (7) + SEC embeddings (384)\n",
    "output_size = 3  # 3 classes: Up, Down, Stable\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GRUStockModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "\n",
    "# âœ… Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# âœ… Train Model\n",
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        if torch.isnan(batch_X).any():\n",
    "            continue\n",
    "        \n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# âœ… Save Training Loss Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"train_loss_with_filings.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nâœ… Training complete. Training loss visualization saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "torch.save(model.state_dict(), \"stock_model_rag.pth\")\n",
    "print(\"âœ… Model saved as 'stock_model_rag.pth'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_csv_with_embeddings(csv_file):\n",
    "    \"\"\"\n",
    "    Process a stock CSV file, compute SEC filing embeddings, and save the modified file.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the stock CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    # âœ… Load CSV\n",
    "    # If file is empty skip\n",
    "    if os.stat(file).st_size == 0:\n",
    "        return # Skip if file is empty\n",
    "    \n",
    "    df = pd.read_csv(file)\n",
    "    if not {'Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation'}.issubset(df.columns):\n",
    "        return  # Skip if missing columns\n",
    "\n",
    "    if \"embedding\" in df.columns:\n",
    "        df.drop(columns=[\"embedding\"], inplace=True)  # Remove existing embeddings\n",
    "\n",
    "    # âœ… Initialize embedding column with zero vector\n",
    "    df[\"embedding\"] = np.zeros((len(df), 384)).tolist()\n",
    "\n",
    "    # âœ… Process each row with 10-K or DEF 14A\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {os.path.basename(csv_file)}\"):\n",
    "        ten_k_path, def_14a_path = str(row[\"10-K\"]), str(row[\"DEF 14A\"])\n",
    "        embedding = None\n",
    "\n",
    "        # âœ… Process 10-K\n",
    "        if ten_k_path != \"0\" and isinstance(ten_k_path, str):\n",
    "            file_path = switch_file_path(ten_k_path)\n",
    "            if os.path.exists(file_path):\n",
    "                embedding = retrieve_avg_embedding(file_path)\n",
    "                df.at[i, \"embedding\"] = embedding.tolist()  # Convert numpy array to list for CSV storage\n",
    "\n",
    "        # âœ… Process DEF 14A if 10-K is missing\n",
    "        elif def_14a_path != \"0\" and isinstance(def_14a_path, str):\n",
    "            file_path = switch_file_path(def_14a_path)\n",
    "            if os.path.exists(file_path):\n",
    "                embedding = retrieve_avg_embedding(file_path)\n",
    "                df.at[i, \"embedding\"] = embedding.tolist()  # Convert numpy array to list for CSV storage\n",
    "\n",
    "    # âœ… Save updated CSV\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"âœ… Embeddings added and saved for {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 0000831259.csv:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2592/6289 [01:55<05:55, 10.39it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# âœ… Path to stock data folder\n",
    "stock_folder = \"/Users/colbywang/Google Drive/æˆ‘çš„äº‘ç«¯ç¡¬ç›˜/Advanced NLP/Assignments/data files/organized/stock-data\"\n",
    "all_files = glob(os.path.join(stock_folder, \"*.csv\"))\n",
    "\n",
    "# âœ… Process each stock CSV file\n",
    "for file in all_files:\n",
    "    process_csv_with_embeddings(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
