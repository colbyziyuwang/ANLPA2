{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to update parent_folder and colab_base as you see fit\n",
    "\n",
    "Download Ollama first to access llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global paths for both local (Mac) and Google Colab\n",
    "PARENT_FOLDER = \"/Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/\"\n",
    "COLAB_BASE = \"/content/gdrive/MyDrive/Assignments/Advanced NLP/Assignments/data files/organized/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class StockLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(StockLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to predict stock percentage change\n",
    "        self.fc = nn.Linear(hidden_size, 7)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # LSTM forward pass\n",
    "        output, hidden = self.lstm(x, hidden)  \n",
    "        \n",
    "        # Take the last output step for prediction\n",
    "        output = self.fc(output[:, -1, :])  \n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden and cell states with zeros\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return (h_0, c_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colbywang/Desktop/ANLPA2/rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load FinBERT model and tokenizer\n",
    "model_name = \"yiyanghkust/finbert-pretrain\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_average_embedding(sentences):\n",
    "    \"\"\"Compute and average sentence embeddings using FinBERT.\"\"\"\n",
    "    embeddings = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        # Forward pass to get hidden states\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Extract [CLS] token embedding\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "        embeddings.append(cls_embedding)\n",
    "\n",
    "    # Convert list to NumPy array and compute the mean embedding\n",
    "    avg_embedding = np.mean(np.array(embeddings), axis=0)\n",
    "    \n",
    "    return avg_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings\n",
    ")\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.packs.sentence_window_retriever import SentenceWindowRetrieverPack as SentenceWindowRetriever\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "# ✅ Load the LLM Model\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2\",\n",
    "    context_window=4096,\n",
    "    request_timeout=600.0,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# ✅ Load the embedding model\n",
    "embedding_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# ✅ Configure Settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_model\n",
    "\n",
    "# ✅ Load documents\n",
    "file_path = \"2001_0000912057-01-006039.txt\"\n",
    "docs = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "\n",
    "# ✅ Create Node Parser with Sentence Window\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=1,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\"\n",
    ")\n",
    "\n",
    "# ✅ Process nodes from documents\n",
    "nodes = node_parser.get_nodes_from_documents(docs)\n",
    "\n",
    "# ✅ Create Vector Store Index\n",
    "index = VectorStoreIndex(nodes)\n",
    "\n",
    "# ✅ Create Retriever\n",
    "retriever = index.as_retriever(\n",
    "    similarity_top_k=3\n",
    ")\n",
    "\n",
    "# ✅ Create Query Engine\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever)\n",
    "\n",
    "# ✅ Function to run queries\n",
    "def run_rag_query(query_text):\n",
    "    response = query_engine.query(query_text)\n",
    "    print(\"\\n🔹 Query:\", query_text)\n",
    "    print(\"\\n🔹 RAG Response:\")\n",
    "    print(response)\n",
    "    return response\n",
    "\n",
    "# ✅ Example usage\n",
    "query = \"What are the top 3-5 material risk factors highlighted in this 10-K?\"\n",
    "response = run_rag_query(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual RAG Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings\n",
    ")\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.packs.sentence_window_retriever import SentenceWindowRetrieverPack as SentenceWindowRetriever\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "# ✅ Load the LLM Model\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2\",\n",
    "    context_window=4096,\n",
    "    request_timeout=600.0,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# ✅ Load the embedding model\n",
    "embedding_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# ✅ Configure Settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_model\n",
    "\n",
    "# ✅ Create Node Parser with Sentence Window (Used in Function)\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=1,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\"\n",
    ")\n",
    "\n",
    "def run_rag_pipeline(file_path, query_text):\n",
    "    \"\"\"\n",
    "    Runs the RAG pipeline for a given document file path and query.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the 10-K or DEF 14A file.\n",
    "        query_text (str): The query to ask the LLM.\n",
    "\n",
    "    Returns:\n",
    "        str: The retrieved response from the document.\n",
    "    \"\"\"\n",
    "\n",
    "    # ✅ Load document\n",
    "    docs = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "\n",
    "    # ✅ Process nodes from document\n",
    "    nodes = node_parser.get_nodes_from_documents(docs)\n",
    "\n",
    "    # ✅ Create Vector Store Index\n",
    "    index = VectorStoreIndex(nodes)\n",
    "\n",
    "    # ✅ Create Retriever\n",
    "    retriever = index.as_retriever(\n",
    "        similarity_top_k=3\n",
    "    )\n",
    "\n",
    "    # ✅ Create Query Engine\n",
    "    query_engine = RetrieverQueryEngine(retriever=retriever)\n",
    "\n",
    "    # ✅ Run the query\n",
    "    response = query_engine.query(query_text)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_file_path(colab_path):\n",
    "    \"\"\"\n",
    "    Converts a file path from Google Drive (Colab) to a local path.\n",
    "\n",
    "    Args:\n",
    "        colab_path (str): The file path from Google Drive in Colab.\n",
    "\n",
    "    Returns:\n",
    "        str: The equivalent local path.\n",
    "    \"\"\"\n",
    "    local_path = colab_path.replace(COLAB_BASE, PARENT_FOLDER, 1)\n",
    "    return local_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 397 files and testing on 100 files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_folder = os.path.join(PARENT_FOLDER, \"stock-data\")\n",
    "csv_files = [file for file in os.listdir(csv_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "# Train/Test split\n",
    "train_files = csv_files[:int(0.8 * len(csv_files))]\n",
    "test_files = csv_files[int(0.8 * len(csv_files)):]\n",
    "print(f\"Training on {len(train_files)} files and testing on {len(test_files)} files.\")\n",
    "\n",
    "def sample_stock_file(files):\n",
    "    # Randomly select one CSV file\n",
    "    stock_file = os.path.join(csv_folder, random.choice(files))\n",
    "\n",
    "    return stock_file\n",
    "\n",
    "def sample_a_row_with_10K_DEF14A(stock_file, num_trading_days=6):\n",
    "    \"\"\"\n",
    "    Randomly selects a row where either a 10-K or DEF 14A filing exists\n",
    "    and returns the row along with the next `num_trading_days` valid trading days.\n",
    "    \"\"\"\n",
    "    # Load the stock data CSV file\n",
    "    stock_data = pd.read_csv(stock_file, parse_dates=[\"Date\"])\n",
    "    stock_data.set_index(\"Date\", inplace=True)  # Ensure Date is the index\n",
    "    stock_data.sort_index(inplace=True)  # Sort by Date to ensure correctness\n",
    "\n",
    "    # First determine randomly whether to sample 10-K or DEF 14A\n",
    "    sample_10K = random.choice([True, False])\n",
    "\n",
    "    # Get the sample row\n",
    "    if sample_10K:\n",
    "        filtered_df = stock_data[stock_data[\"10-K\"] != \"0\"]  # Ensure paths are considered\n",
    "    else:\n",
    "        filtered_df = stock_data[stock_data[\"DEF 14A\"] != \"0\"]\n",
    "\n",
    "    # If no matching row found, return None\n",
    "    if filtered_df.empty:\n",
    "        print(\"⚠️ No matching rows found.\")\n",
    "        return None\n",
    "\n",
    "    # Randomly sample a row\n",
    "    sampled_row = filtered_df.sample(1)\n",
    "    sampled_date = sampled_row.index[0]\n",
    "\n",
    "    # Find the position of this row in the stock DataFrame\n",
    "    sampled_idx = stock_data.index.get_loc(sampled_date)\n",
    "\n",
    "    # Select the next `num_trading_days` rows after sampled index\n",
    "    future_dates = stock_data.index[sampled_idx: sampled_idx + num_trading_days + 1]  # +1 to include sampled row\n",
    "    selected_rows = stock_data.loc[future_dates]\n",
    "\n",
    "    return selected_rows\n",
    "\n",
    "def train(LSTM_Model, stock_file, optimizer, num_trading_days=6):\n",
    "    \"\"\"\n",
    "    Trains the LSTM model on a randomly sampled row from the stock data CSV file.\n",
    "    \n",
    "    Args:\n",
    "        LSTM_Model: The LSTM model.\n",
    "        stock_file: Path to the stock data CSV file.\n",
    "        optimizer: The optimizer for training the model.\n",
    "        num_trading_days: Number of past trading days used as input.\n",
    "\n",
    "    Returns:\n",
    "        loss_value: The computed training loss.\n",
    "    \"\"\"\n",
    "    # Sample a row with 10-K or DEF 14A\n",
    "    stock_data = sample_a_row_with_10K_DEF14A(stock_file, num_trading_days)\n",
    "\n",
    "    # **If no valid data is found, skip training**\n",
    "    if stock_data is None or stock_data.empty:\n",
    "        print(\"⚠️ No valid row found for training. Skipping...\")\n",
    "        return None\n",
    "\n",
    "    # **Extract input features (ignore categorical columns)**\n",
    "    features = stock_data.drop(columns=[\"10-K\", \"DEF 14A\", \"Change\"]).values\n",
    "    target = stock_data[\"Percentage Change\"].values\n",
    "\n",
    "    # **Enhance features with RAG embeddings**\n",
    "    first_row = stock_data.iloc[0]  # First row contains the filing path\n",
    "\n",
    "    # Ensure the values are strings before checking\n",
    "    ten_k_path = str(first_row[\"10-K\"]) if not pd.isna(first_row[\"10-K\"]) else \"0\"\n",
    "    def_14a_path = str(first_row[\"DEF 14A\"]) if not pd.isna(first_row[\"DEF 14A\"]) else \"0\"\n",
    "\n",
    "    # **Check if there is a valid filing path**\n",
    "    if ten_k_path != \"0\":\n",
    "        file_path = switch_file_path(ten_k_path)\n",
    "        response = run_rag_pipeline(file_path, \n",
    "            \"Summarize the most critical financial risks and uncertainties outlined in this 10-K filing.\"\n",
    "        )\n",
    "    elif def_14a_path != \"0\":\n",
    "        file_path = switch_file_path(def_14a_path)\n",
    "        response = run_rag_pipeline(file_path, \n",
    "            \"Summarize the key executive compensation decisions and governance changes disclosed in this DEF 14A filing.\"\n",
    "        )\n",
    "    else:\n",
    "        response = \"\"  # No filing path available, return empty response\n",
    "\n",
    "    # **Convert RAG response into embeddings**\n",
    "    if response:\n",
    "        # **Convert RAG response into embeddings**\n",
    "        response_text = str(response)  # Convert response object to string\n",
    "        \n",
    "        # Ensure response is processed correctly\n",
    "        if hasattr(response, 'response'):  # If response has a .response attribute\n",
    "            response_text = response.response  \n",
    "        elif hasattr(response, 'text'):  # If response has a .text attribute\n",
    "            response_text = response.text  \n",
    "        \n",
    "        rag_sentences = response_text.split(\".\")  # Split into sentences\n",
    "        rag_embedding = get_average_embedding(rag_sentences)  # (768,)\n",
    "        rag_embedding = np.tile(rag_embedding, (features.shape[0], 1))  # Repeat embedding for each row\n",
    "\n",
    "    else:\n",
    "        rag_embedding = np.zeros((features.shape[0], 768))  # Use zero vector if no RAG data\n",
    "\n",
    "    # **Concatenate RAG embedding with features**\n",
    "    features = np.hstack((features, rag_embedding))\n",
    "\n",
    "    # **Normalize the features and target**\n",
    "    features = (features - features.mean(axis=0)) / (features.std(axis=0) + 1e-8)  # Avoid division by zero\n",
    "    target = (target - target.mean()) / (target.std() + 1e-8)\n",
    "\n",
    "    # **Convert to PyTorch tensors**\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0)  # (batch, seq_len, input_size)\n",
    "    target_tensor = torch.tensor(target, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # **Initialize hidden states**\n",
    "    hidden = LSTM_Model.init_hidden(batch_size=1)\n",
    "\n",
    "    # **Forward pass**\n",
    "    output, hidden = LSTM_Model(features_tensor, hidden)\n",
    "\n",
    "    # **Compute loss**\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = loss_fn(output, target_tensor)\n",
    "\n",
    "    # **Backpropagation**\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 2/10000 [13:47<1148:45:59, 413.64s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(stock_files) \n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stock_file \u001b[38;5;129;01min\u001b[39;00m tqdm(stock_files, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 47\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_LSTM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstock_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_trading_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_trading_days\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# If training was successful\u001b[39;00m\n\u001b[1;32m     50\u001b[0m         epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[47], line 100\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(LSTM_Model, stock_file, optimizer, num_trading_days)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m def_14a_path \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     99\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m switch_file_path(def_14a_path)\n\u001b[0;32m--> 100\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrun_rag_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSummarize the key executive compensation decisions and governance changes disclosed in this DEF 14A filing.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# No filing path available, return empty response\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[44], line 52\u001b[0m, in \u001b[0;36mrun_rag_pipeline\u001b[0;34m(file_path, query_text)\u001b[0m\n\u001b[1;32m     49\u001b[0m docs \u001b[38;5;241m=\u001b[39m SimpleDirectoryReader(input_files\u001b[38;5;241m=\u001b[39m[file_path])\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# ✅ Process nodes from document\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m nodes \u001b[38;5;241m=\u001b[39m \u001b[43mnode_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_nodes_from_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# ✅ Create Vector Store Index\u001b[39;00m\n\u001b[1;32m     55\u001b[0m index \u001b[38;5;241m=\u001b[39m VectorStoreIndex(nodes)\n",
      "File \u001b[0;32m~/Desktop/ANLPA2/rag/lib/python3.11/site-packages/llama_index/core/node_parser/interface.py:166\u001b[0m, in \u001b[0;36mNodeParser.get_nodes_from_documents\u001b[0;34m(self, documents, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    163\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mNODE_PARSING, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mDOCUMENTS: documents}\n\u001b[1;32m    164\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[1;32m    165\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_nodes(documents, show_progress\u001b[38;5;241m=\u001b[39mshow_progress, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 166\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_postprocess_parsed_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_id_to_document\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     event\u001b[38;5;241m.\u001b[39mon_end({EventPayload\u001b[38;5;241m.\u001b[39mNODES: nodes})\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n",
      "File \u001b[0;32m~/Desktop/ANLPA2/rag/lib/python3.11/site-packages/llama_index/core/node_parser/interface.py:99\u001b[0m, in \u001b[0;36mNodeParser._postprocess_parsed_nodes\u001b[0;34m(self, nodes, parent_doc_map)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parent_doc\u001b[38;5;241m.\u001b[39msource_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     node\u001b[38;5;241m.\u001b[39mrelationships\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m     95\u001b[0m         {\n\u001b[1;32m     96\u001b[0m             NodeRelationship\u001b[38;5;241m.\u001b[39mSOURCE: parent_doc\u001b[38;5;241m.\u001b[39msource_node,\n\u001b[1;32m     97\u001b[0m         }\n\u001b[1;32m     98\u001b[0m     )\n\u001b[0;32m---> 99\u001b[0m start_char_idx \u001b[38;5;241m=\u001b[39m \u001b[43mparent_doc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMetadataMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNONE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# update start/end char idx\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_char_idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, TextNode):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create Checkpoint Folder if it doesn't exist\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# ✅ Define Model & Optimizer\n",
    "input_size = 785  # (Stock prices & Volume) + (Yield) + (CPI and Inflation) + (RAG embedding)\n",
    "hidden_size = 64\n",
    "num_epochs = 100  # Change this for more training\n",
    "num_trading_days = 6  # Lookback window\n",
    "\n",
    "# ✅ Initialize Model (Load from checkpoint if exists)\n",
    "model_LSTM = StockLSTM(input_size, hidden_size).to(device)\n",
    "\n",
    "# Load from checkpoint if available\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"lstm_final.pth\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model_LSTM.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(f\"🔍 Loaded model checkpoint: {checkpoint_path}\")\n",
    "\n",
    "# ✅ Define Optimizer\n",
    "optimizer = torch.optim.Adam(model_LSTM.parameters(), lr=0.001)\n",
    "\n",
    "# ✅ Training Loop\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n🚀 Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    epoch_loss = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    # **Iterate Through Stock Files**\n",
    "    num_iterations = 10000  # Number of iterations per epoch\n",
    "    stock_files = [sample_stock_file(train_files) for _ in range(num_iterations)]\n",
    "\n",
    "    # 🔀 Shuffle the stock files to introduce randomness\n",
    "    random.shuffle(stock_files) \n",
    "\n",
    "    for stock_file in tqdm(stock_files, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        loss = train(model_LSTM, stock_file, optimizer, num_trading_days=num_trading_days)\n",
    "        \n",
    "        if loss is not None:  # If training was successful\n",
    "            epoch_loss += loss\n",
    "            num_samples += 1\n",
    "\n",
    "    # **Compute Average Loss for Epoch**\n",
    "    avg_loss = epoch_loss / max(num_samples, 1)  # Avoid division by zero\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"✅ Epoch {epoch + 1} - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # **🔹 Save Model Checkpoint Every 2 Epochs**\n",
    "    if epoch % 2 == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"lstm_epoch_{epoch + 1}.pth\")\n",
    "        torch.save(model_LSTM.state_dict(), checkpoint_path)\n",
    "        print(f\"📌 Model saved: {checkpoint_path}\")\n",
    "\n",
    "# ✅ Final Model Save\n",
    "final_model_path = os.path.join(checkpoint_dir, \"lstm_final.pth\")\n",
    "torch.save(model_LSTM.state_dict(), final_model_path)\n",
    "print(f\"🎯 Training Completed! Final model saved: {final_model_path}\")\n",
    "\n",
    "# ✅ Plot the Loss Curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(losses, label=\"Training Loss\", color='blue')\n",
    "plt.title(\"LSTM Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()  # Display the plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
