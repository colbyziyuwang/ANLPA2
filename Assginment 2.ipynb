{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to update parent_folder and colab_base as you see fit\n",
    "\n",
    "Download Ollama first to access llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global paths for both local (Mac) and Google Colab\n",
    "PARENT_FOLDER = \"/Users/colbywang/Google Drive/ÊàëÁöÑ‰∫ëÁ´ØÁ°¨Áõò/Advanced NLP/Assignments/data files/organized/\"\n",
    "COLAB_BASE = \"/content/gdrive/MyDrive/Assignments/Advanced NLP/Assignments/data files/organized/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# ‚úÖ Initialize Embedding Model (Same as Before)\n",
    "embedding_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "Settings.embed_model = embedding_model  # Apply globally\n",
    "\n",
    "# ‚úÖ Initialize Sentence Window Parser (Used for Retrieval)\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=1,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\"\n",
    ")\n",
    "\n",
    "def build_vector_index(file_path):\n",
    "    \"\"\"\n",
    "    Build a vector index from a given document.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the document.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: Built vector store index.\n",
    "    \"\"\"\n",
    "    # ‚úÖ Load document\n",
    "    docs = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "\n",
    "    # ‚úÖ Process nodes from document\n",
    "    nodes = node_parser.get_nodes_from_documents(docs)\n",
    "\n",
    "    # ‚úÖ Create Vector Store Index\n",
    "    index = VectorStoreIndex(nodes)\n",
    "\n",
    "    return index\n",
    "\n",
    "def retrieve_avg_embedding(query_text, vector_index, top_k=3):\n",
    "    \"\"\"\n",
    "    Retrieve top-K relevant passages and compute their average embedding.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The query to search in the vector database.\n",
    "        vector_index (VectorStoreIndex): Prebuilt vector store index.\n",
    "        top_k (int): Number of retrieved passages to consider.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The averaged embedding of retrieved contexts.\n",
    "    \"\"\"\n",
    "    # Retrieve top-K most similar documents\n",
    "    retriever = vector_index.as_retriever(similarity_top_k=top_k)\n",
    "    retrieved_docs = retriever.retrieve(query_text)\n",
    "\n",
    "    # Extract embeddings of retrieved docs\n",
    "    retrieved_embeddings = np.array([\n",
    "        doc.embedding for doc in retrieved_docs if doc.embedding is not None\n",
    "    ])\n",
    "\n",
    "    # Handle case where no embeddings were retrieved\n",
    "    if retrieved_embeddings.size == 0:\n",
    "        print(\"‚ö†Ô∏è No valid embeddings found. Returning zero vector.\")\n",
    "        return np.zeros((embedding_model.get_embedding_dimension(),))\n",
    "\n",
    "    # Compute the average embedding\n",
    "    avg_embedding = np.mean(retrieved_embeddings, axis=0)\n",
    "\n",
    "    return avg_embedding # shape: (384, )\n",
    "\n",
    "# ‚úÖ Example Usage\n",
    "# file_path = \"path/to/your/10-K_or_DEF14A_file.txt\"\n",
    "# query_text = \"Summarize key financial risks.\"\n",
    "\n",
    "# # Build index\n",
    "# vector_index = build_vector_index(file_path)\n",
    "\n",
    "# # Retrieve average embedding\n",
    "# avg_embedding = retrieve_avg_embedding(query_text, vector_index)\n",
    "# print(\"Avg Embedding Shape:\", avg_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_file_path(colab_path):\n",
    "    \"\"\"\n",
    "    Converts a file path from Google Drive (Colab) to a local path.\n",
    "\n",
    "    Args:\n",
    "        colab_path (str): The file path from Google Drive in Colab.\n",
    "\n",
    "    Returns:\n",
    "        str: The equivalent local path.\n",
    "    \"\"\"\n",
    "    local_path = colab_path.replace(COLAB_BASE, PARENT_FOLDER, 1)\n",
    "    return local_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 347 files and testing on 108 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Stock Data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 347/347 [00:06<00:00, 52.91it/s]\n",
      "Processing Stock Data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 108/108 [00:00<00:00, 118026.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Epoch [1/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59992/59992 [02:56<00:00, 340.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 5.1649\n",
      "üîµ Epoch [2/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59992/59992 [02:51<00:00, 348.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 5.1641\n",
      "üîµ Epoch [3/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59992/59992 [02:51<00:00, 349.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Loss: 5.1637\n",
      "üîµ Epoch [4/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59992/59992 [03:02<00:00, 329.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Loss: 5.1631\n",
      "üîµ Epoch [5/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59992/59992 [03:06<00:00, 322.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 5.1627\n",
      "üîµ Epoch [6/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59992/59992 [03:09<00:00, 315.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Loss: 5.1624\n",
      "üîµ Epoch [7/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59992/59992 [03:05<00:00, 323.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Loss: 5.1624\n",
      "üîµ Epoch [8/100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training batches:  31%|‚ñà‚ñà‚ñà       | 18587/59992 [00:56<02:10, 317.23it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ‚úÖ Define Parent Folder for Stock Data\n",
    "PARENT_FOLDER = \"/Users/colbywang/Google Drive/ÊàëÁöÑ‰∫ëÁ´ØÁ°¨Áõò/Advanced NLP/Assignments/data files/organized/stock-data/\"\n",
    "\n",
    "# ‚úÖ Get list of all CSV files in stock data folder\n",
    "csv_files = [os.path.join(PARENT_FOLDER, file) for file in os.listdir(PARENT_FOLDER) if file.endswith(\".csv\")]\n",
    "\n",
    "# ‚úÖ Train/Test Split (80% train, 20% test)\n",
    "train_files = csv_files[:int(0.7 * len(csv_files))]\n",
    "test_files = csv_files[int(0.7 * len(csv_files))]\n",
    "print(f\"Training on {len(train_files)} files and testing on {len(test_files)} files.\")\n",
    "\n",
    "# ‚úÖ Define Function to Load & Preprocess Stock Data\n",
    "def load_and_preprocess_data(stock_file):\n",
    "    # Check if file is empty\n",
    "    if os.stat(stock_file).st_size == 0:\n",
    "        return None, None\n",
    "\n",
    "    df = pd.read_csv(stock_file, parse_dates=[\"Date\"])\n",
    "    df.set_index(\"Date\", inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # ‚úÖ Select Features\n",
    "    features = df[[\"Close\", \"High\", \"Low\", \"Open\", \"Volume\", \"Percentage Change\", \"CPI\", \"Inflation\",\n",
    "                   \"3 Mo\", \"6 Mo\", \"1 Yr\", \"2 Yr\", \"3 Yr\", \"5 Yr\", \"7 Yr\", \"10 Yr\", \"20 Yr\"]]\n",
    "\n",
    "    # ‚úÖ Shift the Target (Predict next day's percentage change)\n",
    "    df[\"Target\"] = df[\"Percentage Change\"].shift(-1)\n",
    "\n",
    "    # ‚úÖ Drop rows with NaN values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return features, df[\"Target\"]\n",
    "\n",
    "# ‚úÖ Define GRU Model\n",
    "class StockGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(StockGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        output = self.fc(output[:, -1, :])  # Take last timestep output\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "# ‚úÖ Device Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ‚úÖ Prepare Data Function\n",
    "def prepare_data(stock_files, sequence_length=10):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for stock_file in tqdm(stock_files, desc=\"Processing Stock Data\"):\n",
    "        if not os.path.isfile(stock_file):  # ‚úÖ Ensure it's a file\n",
    "            continue  # Skip directories\n",
    "\n",
    "        features, target = load_and_preprocess_data(stock_file)\n",
    "\n",
    "        if features is None or target is None:\n",
    "            continue\n",
    "        features, target = features.values, target.values\n",
    "\n",
    "        for i in range(len(features) - sequence_length):\n",
    "            if i + sequence_length >= len(target):  # Ensure index is within bounds\n",
    "                break\n",
    "            X.append(features[i:i + sequence_length])\n",
    "            y.append(target[i + sequence_length])\n",
    "\n",
    "\n",
    "    # Convert X and y to NumPy arrays with explicit dtype\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_tensor = torch.from_numpy(X).to(device)\n",
    "    y_tensor = torch.from_numpy(y).to(device)\n",
    "\n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# ‚úÖ Prepare Training & Testing Data\n",
    "sequence_length = 10\n",
    "X_train, y_train = prepare_data(train_files, sequence_length)\n",
    "X_test, y_test = prepare_data(test_files, sequence_length)\n",
    "\n",
    "# ‚úÖ Initialize GRU Model\n",
    "input_size = X_train.shape[2]\n",
    "hidden_size = 64\n",
    "gru_model = StockGRU(input_size, hidden_size).to(device)\n",
    "\n",
    "# ‚úÖ Define Loss, Optimizer, and Training Parameters\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(gru_model.parameters(), lr=0.001)\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "training_losses = []  # Store losses\n",
    "\n",
    "# ‚úÖ Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"üîµ Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    total_loss = 0\n",
    "    counter = 0\n",
    "    hidden = gru_model.init_hidden(batch_size)\n",
    "\n",
    "    for i in tqdm(range(0, len(X_train), batch_size), desc=\"training batches\"):\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "\n",
    "        if len(batch_X) < batch_size:\n",
    "            continue\n",
    "        \n",
    "        # If batch_X have nan values then continue\n",
    "        if torch.isnan(batch_X).any():\n",
    "            continue\n",
    "\n",
    "        hidden = hidden.detach()\n",
    "\n",
    "        # Forward Pass\n",
    "        output, hidden = gru_model(batch_X, hidden)\n",
    "\n",
    "        # If batch_y have nan values then continue\n",
    "        if torch.isnan(batch_y).any():\n",
    "            continue\n",
    "\n",
    "        loss = loss_fn(output.squeeze(), batch_y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        counter += 1\n",
    "\n",
    "    avg_loss = total_loss / counter\n",
    "    training_losses.append(avg_loss)  # Store loss\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ‚úÖ Save Model\n",
    "torch.save(gru_model.state_dict(), \"stock_gru_model.pth\")\n",
    "\n",
    "# ‚úÖ Plot & Save Training Loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, num_epochs + 1), training_losses, marker=\"o\", linestyle=\"-\", color=\"blue\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss (MSE)\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.savefig(\"training_loss.png\")\n",
    "\n",
    "# ‚úÖ Evaluation on Test Set\n",
    "gru_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_hidden = gru_model.init_hidden(X_test.size(0))\n",
    "    y_pred, _ = gru_model(X_test, test_hidden)\n",
    "    y_pred = y_pred.cpu().numpy().flatten()\n",
    "    y_test = y_test.cpu().numpy().flatten()\n",
    "\n",
    "    # ‚úÖ Calculate Performance Metrics\n",
    "    mae = np.mean(np.abs(y_pred - y_test))\n",
    "    mse = np.mean((y_pred - y_test) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    print(\"\\nüîπ **Test Set Performance Metrics** üîπ\")\n",
    "    print(f\"üìâ Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"üìä Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"üìà Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "    # ‚úÖ Select a Random Test Example for Visualization\n",
    "    random_idx = np.random.randint(0, len(y_test) - 20)  # Pick a random starting point\n",
    "    actual_values = y_test[random_idx:random_idx + 20]\n",
    "    predicted_values = y_pred[random_idx:random_idx + 20]\n",
    "\n",
    "    # ‚úÖ Plot & Save Actual vs Predicted\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(actual_values, label=\"Actual\", marker=\"o\", linestyle=\"-\", color=\"green\")\n",
    "    plt.plot(predicted_values, label=\"Predicted\", marker=\"x\", linestyle=\"--\", color=\"red\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Stock Percentage Change\")\n",
    "    plt.title(\"Actual vs Predicted Stock Percentage Change\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.savefig(\"actual_vs_pred.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
