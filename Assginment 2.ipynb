{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parent Folder:\n",
    "\n",
    "/Users/colbywang/Google Drive/ÊàëÁöÑ‰∫ëÁ´ØÁ°¨Áõò/Advanced NLP/Assignments/data files/organized\n",
    "\n",
    "Download Ollama first to access llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class StockLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(StockLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to predict stock percentage change\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # LSTM forward pass\n",
    "        output, hidden = self.lstm(x, hidden)  \n",
    "        \n",
    "        # Take the last output step for prediction\n",
    "        output = self.fc(output[:, -1, :])  \n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden and cell states with zeros\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return (h_0, c_0)\n",
    "\n",
    "# Example Usage\n",
    "input_size = 1 + 3 + 768  # Example: Stock price, 3 economic indicators, context vector\n",
    "hidden_size = 64\n",
    "sequence_length = 7  # 7 days of past data as input\n",
    "batch_size = 16\n",
    "\n",
    "model = StockLSTM(input_size, hidden_size).to(device)\n",
    "hidden = model.init_hidden(batch_size)\n",
    "\n",
    "# Example input tensor (batch_size, sequence_length, input_size)\n",
    "sample_input = torch.randn(batch_size, sequence_length, input_size).to(device)\n",
    "output, hidden = model(sample_input, hidden)\n",
    "\n",
    "print(\"Output Shape:\", output.shape)  # Expected: (batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged FinBERT Sentence Embedding Shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load FinBERT model and tokenizer\n",
    "model_name = \"yiyanghkust/finbert-pretrain\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_average_embedding(sentences):\n",
    "    \"\"\"Compute and average sentence embeddings using FinBERT.\"\"\"\n",
    "    embeddings = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        # Forward pass to get hidden states\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Extract [CLS] token embedding\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "        embeddings.append(cls_embedding)\n",
    "\n",
    "    # Convert list to NumPy array and compute the mean embedding\n",
    "    avg_embedding = np.mean(np.array(embeddings), axis=0)\n",
    "    \n",
    "    return avg_embedding\n",
    "\n",
    "# Example usage: multiple sentences\n",
    "sentences = [\n",
    "    \"The company's revenue increased by 10% last quarter.\",\n",
    "    \"Market trends indicate strong growth in the AI sector.\",\n",
    "    \"Risk factors include inflation and supply chain disruptions.\",\n",
    "]\n",
    "\n",
    "# Compute average embedding\n",
    "avg_embedding = get_average_embedding(sentences)\n",
    "\n",
    "print(\"Averaged FinBERT Sentence Embedding Shape:\", avg_embedding.shape)  # Expected: (768,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings\n",
    ")\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.packs.sentence_window_retriever import SentenceWindowRetrieverPack as SentenceWindowRetriever\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "# ‚úÖ Load the LLM Model using Llama2\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2\",\n",
    "    context_window=4096,\n",
    "    request_timeout=60.0,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# ‚úÖ Load the embedding model\n",
    "embedding_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# ‚úÖ Configure Settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_model\n",
    "\n",
    "# ‚úÖ Load documents\n",
    "# file_path = \"/content/drive/MyDrive/Advanced NLP/Assignments/data files/organized/10-K/0000001800/2001_0000912057-01-006039.txt\"\n",
    "file_path = \"2001_0000912057-01-006039.txt\"\n",
    "docs = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "\n",
    "# ‚úÖ Create Node Parser with Sentence Window\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=1,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\"\n",
    ")\n",
    "\n",
    "# ‚úÖ Process nodes from documents\n",
    "nodes = node_parser.get_nodes_from_documents(docs)\n",
    "\n",
    "# ‚úÖ Create Vector Store Index\n",
    "index = VectorStoreIndex(nodes)\n",
    "\n",
    "# ‚úÖ Create Retriever\n",
    "retriever = index.as_retriever(\n",
    "    similarity_top_k=3\n",
    ")\n",
    "\n",
    "# ‚úÖ Create Query Engine\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever)\n",
    "\n",
    "# ‚úÖ Function to run queries\n",
    "def run_rag_query(query_text):\n",
    "    response = query_engine.query(query_text)\n",
    "    print(\"\\nüîπ Query:\", query_text)\n",
    "    print(\"\\nüîπ RAG Response:\")\n",
    "    print(response)\n",
    "    return response\n",
    "\n",
    "# ‚úÖ Example usage\n",
    "query = \"What are the top 3-5 material risk factors highlighted in this 10-K?\"\n",
    "response = run_rag_query(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual RAG Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings\n",
    ")\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.packs.sentence_window_retriever import SentenceWindowRetrieverPack as SentenceWindowRetriever\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "# ‚úÖ Load the LLM Model using Llama2\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2\",\n",
    "    context_window=4096,\n",
    "    request_timeout=60.0,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# ‚úÖ Load the embedding model\n",
    "embedding_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# ‚úÖ Configure Settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_model\n",
    "\n",
    "# ‚úÖ Create Node Parser with Sentence Window (Used in Function)\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=1,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\"\n",
    ")\n",
    "\n",
    "def run_rag_pipeline(file_path, query_text):\n",
    "    \"\"\"\n",
    "    Runs the RAG pipeline for a given document file path and query.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the 10-K or DEF 14A file.\n",
    "        query_text (str): The query to ask the LLM.\n",
    "\n",
    "    Returns:\n",
    "        str: The retrieved response from the document.\n",
    "    \"\"\"\n",
    "\n",
    "    # ‚úÖ Load document\n",
    "    docs = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "\n",
    "    # ‚úÖ Process nodes from document\n",
    "    nodes = node_parser.get_nodes_from_documents(docs)\n",
    "\n",
    "    # ‚úÖ Create Vector Store Index\n",
    "    index = VectorStoreIndex(nodes)\n",
    "\n",
    "    # ‚úÖ Create Retriever\n",
    "    retriever = index.as_retriever(\n",
    "        similarity_top_k=3\n",
    "    )\n",
    "\n",
    "    # ‚úÖ Create Query Engine\n",
    "    query_engine = RetrieverQueryEngine(retriever=retriever)\n",
    "\n",
    "    # ‚úÖ Run the query\n",
    "    response = query_engine.query(query_text)\n",
    "\n",
    "    print(\"\\nüîπ Query:\", query_text)\n",
    "    print(\"\\nüîπ RAG Response:\")\n",
    "    print(response)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# ‚úÖ Example usage\n",
    "# file_path = \"2001_0000912057-01-006039.txt\"\n",
    "# query = \"What are the top 3-5 material risk factors highlighted in this 10-K?\"\n",
    "# response = run_rag_pipeline(file_path, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Mac File Path: /Users/colbywang/Google Drive/ÊàëÁöÑ‰∫ëÁ´ØÁ°¨Áõò/Advanced NLP/Assignments/data files/organized/10-K/0000001800/2001_0000912057-01-006039.txt\n"
     ]
    }
   ],
   "source": [
    "def switch_file_path(colab_path):\n",
    "    \"\"\"\n",
    "    Converts a file path from Google Drive (Colab) to a local Mac path.\n",
    "\n",
    "    Args:\n",
    "        colab_path (str): The file path from Google Drive in Colab.\n",
    "\n",
    "    Returns:\n",
    "        str: The equivalent local path for Mac.\n",
    "    \"\"\"\n",
    "    colab_base = \"/content/drive/MyDrive/Advanced NLP/Assignments/data files/organized\"\n",
    "    mac_base = \"/Users/colbywang/Google Drive/ÊàëÁöÑ‰∫ëÁ´ØÁ°¨Áõò/Advanced NLP/Assignments/data files/organized\"\n",
    "\n",
    "    if colab_path.startswith(colab_base):\n",
    "        local_path = colab_path.replace(colab_base, mac_base, 1)\n",
    "        return local_path\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Warning: Path does not match expected Colab structure.\")\n",
    "        return colab_path  # Return original path if no match\n",
    "\n",
    "# ‚úÖ Example Usage:\n",
    "colab_file = \"/content/drive/MyDrive/Advanced NLP/Assignments/data files/organized/10-K/0000001800/2001_0000912057-01-006039.txt\"\n",
    "mac_file = switch_file_path(colab_file)\n",
    "\n",
    "print(\"Converted Mac File Path:\", mac_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
