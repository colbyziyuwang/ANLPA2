{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to update parent_folder and colab_base as you see fit\n",
    "\n",
    "Download Ollama first to access llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global paths for both local (Mac) and Google Colab\n",
    "PARENT_FOLDER = \"/Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/\"\n",
    "COLAB_BASE = \"/content/gdrive/MyDrive/Assignments/Advanced NLP/Assignments/data files/organized/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class StockLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(StockLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to predict stock percentage change\n",
    "        self.fc = nn.Linear(hidden_size, 7)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # LSTM forward pass\n",
    "        output, hidden = self.lstm(x, hidden)  \n",
    "        \n",
    "        # Take the last output step for prediction\n",
    "        output = self.fc(output[:, -1, :])  \n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden and cell states with zeros\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return (h_0, c_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colbywang/Desktop/ANLPA2/rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load FinBERT model and tokenizer\n",
    "model_name = \"yiyanghkust/finbert-pretrain\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_average_embedding(sentences):\n",
    "    \"\"\"Compute and average sentence embeddings using FinBERT.\"\"\"\n",
    "    embeddings = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        # Forward pass to get hidden states\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Extract [CLS] token embedding\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "        embeddings.append(cls_embedding)\n",
    "\n",
    "    # Convert list to NumPy array and compute the mean embedding\n",
    "    avg_embedding = np.mean(np.array(embeddings), axis=0)\n",
    "    \n",
    "    return avg_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings\n",
    ")\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.packs.sentence_window_retriever import SentenceWindowRetrieverPack as SentenceWindowRetriever\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "# ✅ Load the LLM Model\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2\",\n",
    "    context_window=4096,\n",
    "    request_timeout=600.0,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# ✅ Load the embedding model\n",
    "embedding_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# ✅ Configure Settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_model\n",
    "\n",
    "# ✅ Load documents\n",
    "file_path = \"2001_0000912057-01-006039.txt\"\n",
    "docs = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "\n",
    "# ✅ Create Node Parser with Sentence Window\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=1,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\"\n",
    ")\n",
    "\n",
    "# ✅ Process nodes from documents\n",
    "nodes = node_parser.get_nodes_from_documents(docs)\n",
    "\n",
    "# ✅ Create Vector Store Index\n",
    "index = VectorStoreIndex(nodes)\n",
    "\n",
    "# ✅ Create Retriever\n",
    "retriever = index.as_retriever(\n",
    "    similarity_top_k=3\n",
    ")\n",
    "\n",
    "# ✅ Create Query Engine\n",
    "query_engine = RetrieverQueryEngine(retriever=retriever)\n",
    "\n",
    "# ✅ Function to run queries\n",
    "def run_rag_query(query_text):\n",
    "    response = query_engine.query(query_text)\n",
    "    print(\"\\n🔹 Query:\", query_text)\n",
    "    print(\"\\n🔹 RAG Response:\")\n",
    "    print(response)\n",
    "    return response\n",
    "\n",
    "# ✅ Example usage\n",
    "query = \"What are the top 3-5 material risk factors highlighted in this 10-K?\"\n",
    "response = run_rag_query(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual RAG Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings\n",
    ")\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.packs.sentence_window_retriever import SentenceWindowRetrieverPack as SentenceWindowRetriever\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "# ✅ Load the LLM Model\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2\",\n",
    "    context_window=4096,\n",
    "    request_timeout=600.0,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# ✅ Load the embedding model\n",
    "embedding_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# ✅ Configure Settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_model\n",
    "\n",
    "# ✅ Create Node Parser with Sentence Window (Used in Function)\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=1,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\"\n",
    ")\n",
    "\n",
    "def run_rag_pipeline(file_path, query_text):\n",
    "    \"\"\"\n",
    "    Runs the RAG pipeline for a given document file path and query.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the 10-K or DEF 14A file.\n",
    "        query_text (str): The query to ask the LLM.\n",
    "\n",
    "    Returns:\n",
    "        str: The retrieved response from the document.\n",
    "    \"\"\"\n",
    "\n",
    "    # ✅ Load document\n",
    "    docs = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "\n",
    "    # ✅ Process nodes from document\n",
    "    nodes = node_parser.get_nodes_from_documents(docs)\n",
    "\n",
    "    # ✅ Create Vector Store Index\n",
    "    index = VectorStoreIndex(nodes)\n",
    "\n",
    "    # ✅ Create Retriever\n",
    "    retriever = index.as_retriever(\n",
    "        similarity_top_k=3\n",
    "    )\n",
    "\n",
    "    # ✅ Create Query Engine\n",
    "    query_engine = RetrieverQueryEngine(retriever=retriever)\n",
    "\n",
    "    # ✅ Run the query\n",
    "    response = query_engine.query(query_text)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_file_path(colab_path):\n",
    "    \"\"\"\n",
    "    Converts a file path from Google Drive (Colab) to a local path.\n",
    "\n",
    "    Args:\n",
    "        colab_path (str): The file path from Google Drive in Colab.\n",
    "\n",
    "    Returns:\n",
    "        str: The equivalent local path.\n",
    "    \"\"\"\n",
    "    local_path = colab_path.replace(COLAB_BASE, PARENT_FOLDER, 1)\n",
    "    return local_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 397 files and testing on 100 files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_folder = os.path.join(PARENT_FOLDER, \"stock-data\")\n",
    "csv_files = [file for file in os.listdir(csv_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "# Train/Test split\n",
    "train_files = csv_files[:int(0.8 * len(csv_files))]\n",
    "test_files = csv_files[int(0.8 * len(csv_files)):]\n",
    "print(f\"Training on {len(train_files)} files and testing on {len(test_files)} files.\")\n",
    "\n",
    "def sample_stock_file(files):\n",
    "    # Randomly select one CSV file\n",
    "    stock_file = os.path.join(csv_folder, random.choice(files))\n",
    "\n",
    "    return stock_file\n",
    "\n",
    "def sample_a_row_with_10K_DEF14A(stock_file, num_trading_days=6):\n",
    "    \"\"\"\n",
    "    Randomly selects a row where either a 10-K or DEF 14A filing exists\n",
    "    and returns the row along with the next `num_trading_days` valid trading days.\n",
    "    \"\"\"\n",
    "    # Load the stock data CSV file\n",
    "    stock_data = pd.read_csv(stock_file, parse_dates=[\"Date\"])\n",
    "    stock_data.set_index(\"Date\", inplace=True)  # Ensure Date is the index\n",
    "    stock_data.sort_index(inplace=True)  # Sort by Date to ensure correctness\n",
    "\n",
    "    # First determine randomly whether to sample 10-K or DEF 14A\n",
    "    sample_10K = random.choice([True, False])\n",
    "\n",
    "    # Get the sample row\n",
    "    if sample_10K:\n",
    "        filtered_df = stock_data[stock_data[\"10-K\"] != \"0\"]  # Ensure paths are considered\n",
    "    else:\n",
    "        filtered_df = stock_data[stock_data[\"DEF 14A\"] != \"0\"]\n",
    "\n",
    "    # If no matching row found, return None\n",
    "    if filtered_df.empty:\n",
    "        print(\"⚠️ No matching rows found.\")\n",
    "        return None\n",
    "\n",
    "    # Randomly sample a row\n",
    "    sampled_row = filtered_df.sample(1)\n",
    "    sampled_date = sampled_row.index[0]\n",
    "\n",
    "    # Find the position of this row in the stock DataFrame\n",
    "    sampled_idx = stock_data.index.get_loc(sampled_date)\n",
    "\n",
    "    # Select the next `num_trading_days` rows after sampled index\n",
    "    future_dates = stock_data.index[sampled_idx: sampled_idx + num_trading_days + 1]  # +1 to include sampled row\n",
    "    selected_rows = stock_data.loc[future_dates]\n",
    "\n",
    "    return selected_rows\n",
    "\n",
    "def train(LSTM_Model, stock_file, optimizer, num_trading_days=6):\n",
    "    \"\"\"\n",
    "    Trains the LSTM model on a randomly sampled row from the stock data CSV file.\n",
    "    \n",
    "    Args:\n",
    "        LSTM_Model: The LSTM model.\n",
    "        stock_file: Path to the stock data CSV file.\n",
    "        optimizer: The optimizer for training the model.\n",
    "        num_trading_days: Number of past trading days used as input.\n",
    "\n",
    "    Returns:\n",
    "        loss_value: The computed training loss.\n",
    "    \"\"\"\n",
    "    # Sample a row with 10-K or DEF 14A\n",
    "    stock_data = sample_a_row_with_10K_DEF14A(stock_file, num_trading_days)\n",
    "\n",
    "    # **If no valid data is found, skip training**\n",
    "    if stock_data is None or stock_data.empty:\n",
    "        print(\"⚠️ No valid row found for training. Skipping...\")\n",
    "        return None\n",
    "\n",
    "    # **Extract input features (ignore categorical columns)**\n",
    "    features = stock_data.drop(columns=[\"10-K\", \"DEF 14A\", \"Change\"]).values\n",
    "    target = stock_data[\"Percentage Change\"].values\n",
    "\n",
    "    # **Enhance features with RAG embeddings**\n",
    "    first_row = stock_data.iloc[0]  # First row contains the filing path\n",
    "\n",
    "    # Ensure the values are strings before checking\n",
    "    ten_k_path = str(first_row[\"10-K\"]) if not pd.isna(first_row[\"10-K\"]) else \"0\"\n",
    "    def_14a_path = str(first_row[\"DEF 14A\"]) if not pd.isna(first_row[\"DEF 14A\"]) else \"0\"\n",
    "\n",
    "    # **Check if there is a valid filing path**\n",
    "    if ten_k_path != \"0\":\n",
    "        file_path = switch_file_path(ten_k_path)\n",
    "        response = run_rag_pipeline(file_path, \n",
    "            \"Summarize the most critical financial risks and uncertainties outlined in this 10-K filing.\"\n",
    "        )\n",
    "    elif def_14a_path != \"0\":\n",
    "        file_path = switch_file_path(def_14a_path)\n",
    "        response = run_rag_pipeline(file_path, \n",
    "            \"Summarize the key executive compensation decisions and governance changes disclosed in this DEF 14A filing.\"\n",
    "        )\n",
    "    else:\n",
    "        response = \"\"  # No filing path available, return empty response\n",
    "\n",
    "    # **Convert RAG response into embeddings**\n",
    "    if response:\n",
    "        # **Convert RAG response into embeddings**\n",
    "        response_text = str(response)  # Convert response object to string\n",
    "        \n",
    "        # Ensure response is processed correctly\n",
    "        if hasattr(response, 'response'):  # If response has a .response attribute\n",
    "            response_text = response.response  \n",
    "        elif hasattr(response, 'text'):  # If response has a .text attribute\n",
    "            response_text = response.text  \n",
    "        \n",
    "        rag_sentences = response_text.split(\".\")  # Split into sentences\n",
    "        rag_embedding = get_average_embedding(rag_sentences)  # (768,)\n",
    "        rag_embedding = np.tile(rag_embedding, (features.shape[0], 1))  # Repeat embedding for each row\n",
    "\n",
    "    else:\n",
    "        rag_embedding = np.zeros((features.shape[0], 768))  # Use zero vector if no RAG data\n",
    "\n",
    "    # **Concatenate RAG embedding with features**\n",
    "    features = np.hstack((features, rag_embedding))\n",
    "\n",
    "    # **Normalize the features and target**\n",
    "    features = (features - features.mean(axis=0)) / (features.std(axis=0) + 1e-8)  # Avoid division by zero\n",
    "    target = (target - target.mean()) / (target.std() + 1e-8)\n",
    "\n",
    "    # **Convert to PyTorch tensors**\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0)  # (batch, seq_len, input_size)\n",
    "    target_tensor = torch.tensor(target, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # **Initialize hidden states**\n",
    "    hidden = LSTM_Model.init_hidden(batch_size=1)\n",
    "\n",
    "    # **Forward pass**\n",
    "    output, hidden = LSTM_Model(features_tensor, hidden)\n",
    "\n",
    "    # **Compute loss**\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = loss_fn(output, target_tensor)\n",
    "\n",
    "    # **Backpropagation**\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/10000 [00:00<?, ?it/s]/Users/colbywang/Desktop/ANLPA2/rag/lib/python3.11/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1, 7, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training Epoch 1:   0%|          | 1/10000 [04:48<801:08:23, 288.44s/it]/Users/colbywang/Desktop/ANLPA2/rag/lib/python3.11/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1, 7, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training Epoch 1:   0%|          | 2/10000 [05:42<418:29:23, 150.69s/it]/Users/colbywang/Desktop/ANLPA2/rag/lib/python3.11/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1, 7, 1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training Epoch 1:   0%|          | 3/10000 [12:33<697:45:48, 251.27s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(stock_files) \n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stock_file \u001b[38;5;129;01min\u001b[39;00m tqdm(stock_files, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 47\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_LSTM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstock_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_trading_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_trading_days\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# If training was successful\u001b[39;00m\n\u001b[1;32m     50\u001b[0m         epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[0;32mIn[41], line 100\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(LSTM_Model, stock_file, optimizer, num_trading_days)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m def_14a_path \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     99\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m switch_file_path(def_14a_path)\n\u001b[0;32m--> 100\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrun_rag_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSummarize the key executive compensation decisions and governance changes disclosed in this DEF 14A filing.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# No filing path available, return empty response\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[44], line 55\u001b[0m, in \u001b[0;36mrun_rag_pipeline\u001b[0;34m(file_path, query_text)\u001b[0m\n\u001b[1;32m     52\u001b[0m nodes \u001b[38;5;241m=\u001b[39m node_parser\u001b[38;5;241m.\u001b[39mget_nodes_from_documents(docs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# ✅ Create Vector Store Index\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# ✅ Create Retriever\u001b[39;00m\n\u001b[1;32m     58\u001b[0m retriever \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mas_retriever(\n\u001b[1;32m     59\u001b[0m     similarity_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     60\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/ANLPA2/rag/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:76\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[0;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     70\u001b[0m     resolve_embed_model(embed_model, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m Settings\u001b[38;5;241m.\u001b[39membed_model\n\u001b[1;32m     73\u001b[0m )\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ANLPA2/rag/lib/python3.11/site-packages/llama_index/core/indices/base.py:77\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[0;34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m---> 77\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
      "File \u001b[0;32m~/Desktop/ANLPA2/rag/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:310\u001b[0m, in \u001b[0;36mVectorStoreIndex.build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(content_nodes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(nodes):\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome nodes are missing content, skipping them...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ANLPA2/rag/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:279\u001b[0m, in \u001b[0;36mVectorStoreIndex._build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     run_async_tasks(tasks)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_nodes_to_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
      "File \u001b[0;32m~/Desktop/ANLPA2/rag/lib/python3.11/site-packages/llama_index/core/indices/vector_store/base.py:244\u001b[0m, in \u001b[0;36mVectorStoreIndex._add_nodes_to_index\u001b[0;34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m         node_without_embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    243\u001b[0m         index_struct\u001b[38;5;241m.\u001b[39madd_node(node_without_embedding, text_id\u001b[38;5;241m=\u001b[39mnew_id)\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_docstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode_without_embedding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# NOTE: if the vector store keeps text,\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# we only need to add image and index nodes\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node, new_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(nodes_batch, new_ids):\n",
      "File \u001b[0;32m~/Desktop/ANLPA2/rag/lib/python3.11/site-packages/llama_index/core/storage/docstore/keyval_docstore.py:223\u001b[0m, in \u001b[0;36mKVDocumentStore.add_documents\u001b[0;34m(self, docs, allow_update, batch_size, store_text)\u001b[0m\n\u001b[1;32m    217\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size\n\u001b[1;32m    219\u001b[0m node_kv_pairs, metadata_kv_pairs, ref_doc_kv_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_kv_pairs(\n\u001b[1;32m    220\u001b[0m     docs, allow_update, store_text\n\u001b[1;32m    221\u001b[0m )\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kvstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput_all\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_kv_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_node_collection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kvstore\u001b[38;5;241m.\u001b[39mput_all(\n\u001b[1;32m    230\u001b[0m     metadata_kv_pairs,\n\u001b[1;32m    231\u001b[0m     collection\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_collection,\n\u001b[1;32m    232\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    233\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kvstore\u001b[38;5;241m.\u001b[39mput_all(\n\u001b[1;32m    236\u001b[0m     ref_doc_kv_pairs,\n\u001b[1;32m    237\u001b[0m     collection\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_doc_collection,\n\u001b[1;32m    238\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    239\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/ANLPA2/rag/lib/python3.11/site-packages/llama_index/core/storage/kvstore/types.py:23\u001b[0m, in \u001b[0;36mBaseKVStore.put_all\u001b[0;34m(self, kv_pairs, collection, batch_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;129m@abstractmethod\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21maput\u001b[39m(\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m, val: \u001b[38;5;28mdict\u001b[39m, collection: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_COLLECTION\n\u001b[1;32m     20\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mput_all\u001b[39m(\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     25\u001b[0m     kv_pairs: List[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]],\n\u001b[1;32m     26\u001b[0m     collection: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_COLLECTION,\n\u001b[1;32m     27\u001b[0m     batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_BATCH_SIZE,\n\u001b[1;32m     28\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# by default, support a batch size of 1\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatching not supported by this key-value store.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create Checkpoint Folder if it doesn't exist\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# ✅ Define Model & Optimizer\n",
    "input_size = 785  # (Stock prices & Volume) + (Yield) + (CPI and Inflation) + (RAG embedding)\n",
    "hidden_size = 64\n",
    "num_epochs = 100  # Change this for more training\n",
    "num_trading_days = 6  # Lookback window\n",
    "\n",
    "# ✅ Initialize Model (Load from checkpoint if exists)\n",
    "model_LSTM = StockLSTM(input_size, hidden_size).to(device)\n",
    "\n",
    "# Load from checkpoint if available\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"lstm_final.pth\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model_LSTM.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(f\"🔍 Loaded model checkpoint: {checkpoint_path}\")\n",
    "\n",
    "# ✅ Define Optimizer\n",
    "optimizer = torch.optim.Adam(model_LSTM.parameters(), lr=0.001)\n",
    "\n",
    "# ✅ Training Loop\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n🚀 Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    epoch_loss = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    # **Iterate Through Stock Files**\n",
    "    num_iterations = 10000  # Number of iterations per epoch\n",
    "    stock_files = [sample_stock_file(train_files) for _ in range(num_iterations)]\n",
    "\n",
    "    # 🔀 Shuffle the stock files to introduce randomness\n",
    "    random.shuffle(stock_files) \n",
    "\n",
    "    for stock_file in tqdm(stock_files, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        loss = train(model_LSTM, stock_file, optimizer, num_trading_days=num_trading_days)\n",
    "        \n",
    "        if loss is not None:  # If training was successful\n",
    "            epoch_loss += loss\n",
    "            num_samples += 1\n",
    "\n",
    "    # **Compute Average Loss for Epoch**\n",
    "    avg_loss = epoch_loss / max(num_samples, 1)  # Avoid division by zero\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"✅ Epoch {epoch + 1} - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # **🔹 Save Model Checkpoint Every 2 Epochs**\n",
    "    if epoch % 2 == 0:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"lstm_epoch_{epoch + 1}.pth\")\n",
    "        torch.save(model_LSTM.state_dict(), checkpoint_path)\n",
    "        print(f\"📌 Model saved: {checkpoint_path}\")\n",
    "\n",
    "# ✅ Final Model Save\n",
    "final_model_path = os.path.join(checkpoint_dir, \"lstm_final.pth\")\n",
    "torch.save(model_LSTM.state_dict(), final_model_path)\n",
    "print(f\"🎯 Training Completed! Final model saved: {final_model_path}\")\n",
    "\n",
    "# ✅ Plot the Loss Curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(losses, label=\"Training Loss\", color='blue')\n",
    "plt.title(\"LSTM Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()  # Display the plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
