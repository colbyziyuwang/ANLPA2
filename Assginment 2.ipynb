{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# GRU Model Definition\n",
    "class GRUStockModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRUStockModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # Take last time step output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to update parent_folder and colab_base as you see fit\n",
    "\n",
    "Download Ollama first to access llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global paths for both local (Mac) and Google Colab\n",
    "PARENT_FOLDER = \"/Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/\"\n",
    "COLAB_BASE = \"/content/gdrive/MyDrive/Assignments/Advanced NLP/Assignments/data files/organized/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_file_path(google_drive_path):\n",
    "    \"\"\"\n",
    "    Convert Google Drive file paths to local file paths.\n",
    "    \n",
    "    Args:\n",
    "        google_drive_path (str): File path stored in the CSV (Google Drive format).\n",
    "    \n",
    "    Returns:\n",
    "        str: Converted local file path.\n",
    "    \"\"\"\n",
    "    google_prefix = \"/content/gdrive/MyDrive/Assignments\"  # Incorrect Google Drive path\n",
    "    local_prefix = \"/Users/colbywang/Google Drive/我的云端硬盘\"  # Correct local prefix\n",
    "\n",
    "    if google_drive_path.startswith(google_prefix):\n",
    "        local_path = google_drive_path.replace(google_prefix, local_prefix)\n",
    "        return local_path\n",
    "    \n",
    "    return google_drive_path  # If not found, return as is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to stock data folder\n",
    "PARENT_FOLDER = \"/Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/stock-data\"\n",
    "all_files = glob(os.path.join(PARENT_FOLDER, \"*.csv\"))\n",
    "\n",
    "# Hyperparameters\n",
    "sequence_length = 7  # Lookback window\n",
    "N = 1  # Predict trend in next N days\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "threshold = 0.02  # 2% change threshold for \"Stable\"\n",
    "train_ratio = 0.7  # 70% of files for training, 30% for testing\n",
    "\n",
    "# Split stock files into training and testing sets\n",
    "train_files, test_files = train_test_split(all_files, test_size=1-train_ratio, random_state=42)\n",
    "print(f\"Training on {len(train_files)} stock files, Testing on {len(test_files)} stock files\")\n",
    "\n",
    "def create_labels(df):\n",
    "    \"\"\"\n",
    "    Create classification labels for stock price movement.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Stock data\n",
    "        N (int): Number of days in the future to compute price movement\n",
    "        threshold (float): Percentage change threshold to classify Up or Down\n",
    "\n",
    "    Returns:\n",
    "        Series: Labels (0 = Stable, 1 = Up, 2 = Down)\n",
    "    \"\"\"\n",
    "    df['pct change'] = df['Close'].pct_change(N)\n",
    "    df['Close_diff_pct'] = df['pct change'].shift(-N)\n",
    "\n",
    "    # Default to \"Stable\"\n",
    "    df['Label'] = 0  \n",
    "\n",
    "    # Up (if change > threshold)\n",
    "    df.loc[df['Close_diff_pct'] > threshold, 'Label'] = 1  \n",
    "\n",
    "    # Down (if change < -threshold)\n",
    "    df.loc[df['Close_diff_pct'] < -threshold, 'Label'] = 2  \n",
    "\n",
    "    return df['Label']\n",
    "\n",
    "def create_sequences(df, labels, sequence_length, N=5):\n",
    "    \"\"\"\n",
    "    Create sequences for GRU training.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Stock features\n",
    "        labels (Series): Classification labels\n",
    "        sequence_length (int): Length of input sequence\n",
    "        N (int): Lookahead period for labels\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: X (features)\n",
    "        np.ndarray: y (labels)\n",
    "    \"\"\"\n",
    "    df = df[['Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation']]\n",
    "    X, y = [], []\n",
    "\n",
    "    # Ensure labels align with future price movement\n",
    "    for i in range(len(df) - sequence_length - N):\n",
    "        X.append(df.values[i:i + sequence_length])  # Sequence of input features\n",
    "        y.append(labels.iloc[i + sequence_length])  # Label for the next movement\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load and preprocess all training stock data\n",
    "all_X_train, all_y_train = [], []\n",
    "\n",
    "for file in tqdm(train_files, desc=\"Loading training files\"):\n",
    "    # If file is empty skip\n",
    "    if os.stat(file).st_size == 0:\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(file)\n",
    "    if not {'Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation'}.issubset(df.columns):\n",
    "        continue  # Skip if missing columns\n",
    "\n",
    "    df = df[['Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation']]\n",
    "\n",
    "    labels = create_labels(df)\n",
    "    X, y = create_sequences(df, labels, sequence_length)\n",
    "\n",
    "    all_X_train.append(X)\n",
    "    all_y_train.append(y)\n",
    "\n",
    "# Concatenate all training sequences\n",
    "X_train = np.concatenate(all_X_train, axis=0)\n",
    "y_train = np.concatenate(all_y_train, axis=0)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "# Create PyTorch DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize Model\n",
    "input_size = 7  # ['Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation']\n",
    "output_size = 3  # 3 classes: Up, Down, Stable\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GRUStockModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train Model\n",
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        # If batch_X has nan skip\n",
    "        if torch.isnan(batch_X).any():\n",
    "            continue\n",
    "        \n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save Training Loss Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"train_loss.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n✅ Training complete. Training loss visualization saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved as 'stock_model.pth'.\n"
     ]
    }
   ],
   "source": [
    "# Save Model\n",
    "torch.save(model.state_dict(), \"stock_gru_model.pth\")\n",
    "print(\"✅ Model saved as 'stock_model.pth'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stock files: 100%|██████████| 150/150 [06:29<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 **Overall Test Set Performance Metrics** 🔹\n",
      "✅ Accuracy: 0.7716\n",
      "📏 Precision: 0.6080\n",
      "📡 Recall: 0.7716\n",
      "⚖️ F1-Score: 0.6726\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# ✅ Define Parent Folder for Stock Data\n",
    "PARENT_FOLDER = \"/Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/stock-data/\"\n",
    "\n",
    "# ✅ Get list of test files\n",
    "csv_files = [os.path.join(PARENT_FOLDER, file) for file in os.listdir(PARENT_FOLDER) if file.endswith(\".csv\")]\n",
    "test_files = csv_files[int(0.7 * len(csv_files)):]\n",
    "\n",
    "# ✅ Load GRU Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GRUStockModel(input_size=7, hidden_size=64, num_layers=2, output_size=3).to(device)\n",
    "model.load_state_dict(torch.load(\"stock_gru_model.pth\"))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# ✅ Initialize Metrics Storage\n",
    "all_actuals, all_predictions = [], []\n",
    "\n",
    "# ✅ Process Each Test Stock\n",
    "for test_stock in tqdm(test_files, desc=\"stock files\"):\n",
    "    # If file is empty skip\n",
    "    if os.stat(test_stock).st_size == 0:\n",
    "        continue\n",
    "\n",
    "    df_test = pd.read_csv(test_stock)\n",
    "    if not {'Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation'}.issubset(df.columns):\n",
    "        continue  # Skip if missing columns\n",
    "\n",
    "    # Select Relevant Features\n",
    "    df_test = df_test[['Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation']]\n",
    "\n",
    "    # Generate Labels\n",
    "    labels_test = create_labels(df_test)\n",
    "\n",
    "    # Create Sequences\n",
    "    X_test, y_test = create_sequences(df_test, labels_test, sequence_length=7)\n",
    "    X_test, y_test = torch.tensor(X_test, dtype=torch.float32).to(device), torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    # ✅ Run Inference\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X_test)):\n",
    "            X_sample = X_test[i].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "            if torch.isnan(X_sample).any():\n",
    "                continue\n",
    "\n",
    "            y_actual = y_test[i].item()\n",
    "            output = model(X_sample)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            # Store results\n",
    "            all_actuals.append(y_actual)\n",
    "            all_predictions.append(predicted.item())\n",
    "\n",
    "# ✅ Compute Classification Metrics\n",
    "accuracy = accuracy_score(all_actuals, all_predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_actuals, all_predictions, average=\"weighted\", zero_division=0)\n",
    "\n",
    "# ✅ Print Results\n",
    "print(\"\\n🔹 **Overall Test Set Performance Metrics** 🔹\")\n",
    "print(f\"✅ Accuracy: {accuracy:.4f}\")\n",
    "print(f\"📏 Precision: {precision:.4f}\")\n",
    "print(f\"📡 Recall: {recall:.4f}\")\n",
    "print(f\"⚖️ F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with a Doc2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 347 stock files, Testing on 150 stock files\n",
      "\n",
      "🟢 Epoch 1/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files:   1%|          | 2/347 [03:44<10:30:54, 109.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/DEF 14A/0000086312/2014_0001104659-14-027287.txt: [Errno 60] Operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files:  12%|█▏        | 40/347 [53:17<6:59:45, 82.04s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/10-K/0001065088/2012_0001065088-12-000006.txt: [Errno 60] Operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files:  13%|█▎        | 45/347 [1:02:42<9:06:23, 108.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/10-K/0000723531/2019_0000723531-19-000032.txt: [Errno 60] Operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files:  34%|███▎      | 117/347 [2:39:46<4:46:34, 74.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/10-K/0000105770/2023_0000105770-23-000012.txt: [Errno 60] Operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files:  46%|████▌     | 159/347 [3:38:15<3:39:29, 70.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/10-K/0001757898/2019_0001757898-19-000005.txt: [Errno 60] Operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files:  51%|█████     | 176/347 [3:59:29<3:27:53, 72.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/10-K/0000702165/2010_0000702165-10-000049.txt: [Errno 60] Operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files:  82%|████████▏ | 286/347 [6:06:28<1:19:25, 78.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/DEF 14A/0001179929/2005_0001193125-05-061431.txt: [Errno 60] Operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files: 100%|██████████| 347/347 [7:20:18<00:00, 76.13s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6693 ✅ Model Saved!\n",
      "\n",
      "🟢 Epoch 2/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files:   2%|▏         | 6/347 [07:39<7:11:47, 75.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/10-K/0000277948/2019_0000277948-19-000011.txt: [Errno 60] Operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files:  93%|█████████▎| 322/347 [6:49:14<26:21, 63.24s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/10-K/0000728535/2013_0001437749-13-001883.txt: [Errno 60] Operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files: 100%|██████████| 347/347 [7:26:07<00:00, 77.14s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.6694 ✅ Model Saved!\n",
      "\n",
      "🟢 Epoch 3/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files:   6%|▌         | 20/347 [26:53<7:26:06, 81.85s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/DEF 14A/0001060009/2006_0001193125-06-068085.txt: [Errno 60] Operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files:  14%|█▍        | 50/347 [1:06:42<8:01:55, 97.36s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/10-K/0000874761/2015_0000874761-15-000009.txt: [Errno 60] Operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files:  35%|███▍      | 120/347 [2:44:45<5:37:42, 89.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/10-K/0001109357/2014_0001193125-14-051838.txt: [Errno 60] Operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files:  54%|█████▍    | 189/347 [4:17:18<3:35:57, 82.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/DEF 14A/0000038777/2023_0001308179-23-001092.txt: [Errno 60] Operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files: 100%|██████████| 347/347 [7:26:12<00:00, 77.15s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.6693 ✅ Model Saved!\n",
      "\n",
      "🟢 Epoch 4/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files:  11%|█         | 37/347 [50:08<6:39:55, 77.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/DEF 14A/0001730168/2019_0001193125-19-043540.txt: [Errno 60] Operation timed out\n",
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/10-K/0001730168/2019_0001730168-19-000144.txt: [Errno 60] Operation timed out\n",
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/DEF 14A/0001730168/2020_0001730168-20-000020.txt: [Errno 60] Operation timed out\n",
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/10-K/0001730168/2020_0001730168-20-000226.txt: [Errno 60] Operation timed out\n",
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/DEF 14A/0001730168/2021_0001730168-21-000017.txt: [Errno 60] Operation timed out\n",
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/10-K/0001730168/2021_0001730168-21-000153.txt: [Errno 60] Operation timed out\n",
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/DEF 14A/0001730168/2022_0001730168-22-000012.txt: [Errno 60] Operation timed out\n",
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/10-K/0001730168/2022_0001730168-22-000118.txt: [Errno 60] Operation timed out\n",
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/DEF 14A/0001730168/2023_0001140361-23-007640.txt: [Errno 60] Operation timed out\n",
      "⚠️ Error retrieving embedding for /Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/10-K/0001730168/2023_0001730168-23-000096.txt: [Errno 60] Operation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files: 100%|██████████| 347/347 [50:14<00:00,  8.69s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.0737 ✅ Model Saved!\n",
      "\n",
      "🟢 Epoch 5/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files: 100%|██████████| 347/347 [00:00<00:00, 125727.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.0000 ✅ Model Saved!\n",
      "\n",
      "🟢 Epoch 6/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files: 100%|██████████| 347/347 [00:00<00:00, 140322.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.0000 ✅ Model Saved!\n",
      "\n",
      "🟢 Epoch 7/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files: 100%|██████████| 347/347 [00:00<00:00, 118230.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.0000 ✅ Model Saved!\n",
      "\n",
      "🟢 Epoch 8/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files: 100%|██████████| 347/347 [00:00<00:00, 141111.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.0000 ✅ Model Saved!\n",
      "\n",
      "🟢 Epoch 9/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files: 100%|██████████| 347/347 [00:00<00:00, 176052.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.0000 ✅ Model Saved!\n",
      "\n",
      "🟢 Epoch 10/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files: 100%|██████████| 347/347 [00:00<00:00, 138969.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.0000 ✅ Model Saved!\n",
      "\n",
      "🟢 Epoch 11/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files: 100%|██████████| 347/347 [00:00<00:00, 142381.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.0000 ✅ Model Saved!\n",
      "\n",
      "🟢 Epoch 12/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files: 100%|██████████| 347/347 [00:00<00:00, 146730.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 0.0000 ✅ Model Saved!\n",
      "\n",
      "🟢 Epoch 13/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files: 100%|██████████| 347/347 [00:00<00:00, 134661.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 0.0000 ✅ Model Saved!\n",
      "\n",
      "🟢 Epoch 14/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files: 100%|██████████| 347/347 [00:00<00:00, 142562.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 0.0000 ✅ Model Saved!\n",
      "\n",
      "🟢 Epoch 15/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files: 100%|██████████| 347/347 [00:00<00:00, 111578.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 0.0000 ✅ Model Saved!\n",
      "\n",
      "🟢 Epoch 16/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files: 100%|██████████| 347/347 [00:00<00:00, 112041.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 0.0000 ✅ Model Saved!\n",
      "\n",
      "🟢 Epoch 17/17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Training Files: 100%|██████████| 347/347 [00:00<00:00, 155245.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 0.0000 ✅ Model Saved!\n",
      "\n",
      "✅ Training complete! Final model saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "# ✅ Load Pretrained Doc2Vec Model\n",
    "doc2vec_model = Doc2Vec.load(\"sec_doc2vec.model\")\n",
    "embedding_dim = doc2vec_model.vector_size  # Get embedding size from Doc2Vec\n",
    "\n",
    "# ✅ Path to stock data folder\n",
    "stock_folder = \"/Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/stock-data\"\n",
    "all_files = glob(os.path.join(stock_folder, \"*.csv\"))\n",
    "\n",
    "# ✅ Hyperparameters\n",
    "sequence_length = 7  # Lookback window\n",
    "N = 1  # Predict trend in next N days\n",
    "batch_size = 32\n",
    "epochs = 17 # 3 epochs already trained\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "threshold = 0.02  # 2% change threshold for \"Stable\"\n",
    "train_ratio = 0.7  # 70% of files for training, 30% for testing\n",
    "save_every_files = 2  # ✅ Save model every 2 stock CSVs\n",
    "\n",
    "# ✅ Split stock files into training and testing sets\n",
    "train_files, test_files = train_test_split(all_files, test_size=1-train_ratio, random_state=42)\n",
    "print(f\"Training on {len(train_files)} stock files, Testing on {len(test_files)} stock files\")\n",
    "\n",
    "def create_labels(df):\n",
    "    \"\"\"Create classification labels for stock price movement.\"\"\"\n",
    "    df['pct change'] = df['Close'].pct_change(N)\n",
    "    df['Close_diff_pct'] = df['pct change'].shift(-N)\n",
    "    df['Label'] = 0  \n",
    "    df.loc[df['Close_diff_pct'] > threshold, 'Label'] = 1  \n",
    "    df.loc[df['Close_diff_pct'] < -threshold, 'Label'] = 2  \n",
    "    return df['Label']\n",
    "\n",
    "def get_filing_embedding(file_path):\n",
    "    \"\"\"Retrieves embedding for a single SEC filing, ensuring shape consistency.\"\"\"\n",
    "    embedding = np.zeros((embedding_dim,), dtype=np.float32)  # Default zero vector\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            embedding = doc2vec_model.infer_vector(text.split())\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error retrieving embedding for {file_path}: {e}\")\n",
    "\n",
    "    # ✅ Ensure embedding has correct shape\n",
    "    if embedding.shape != (embedding_dim,):\n",
    "        print(f\"⚠️ Bad embedding shape: {embedding.shape} for {file_path}. Using zero vector.\")\n",
    "        embedding = np.zeros((embedding_dim,), dtype=np.float32)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "def create_sequences(df, labels, filing_embeddings, sequence_length):\n",
    "    \"\"\"Create sequences for GRU training including SEC filing embeddings.\"\"\"\n",
    "    stock_features = df[['Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation']].values\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(len(stock_features) - sequence_length - N):\n",
    "        stock_seq = stock_features[i:i + sequence_length]  # (seq_len, 7)\n",
    "        filing_seq = filing_embeddings[i:i + sequence_length]  # (seq_len, embedding_dim)\n",
    "\n",
    "        # ✅ Fix potential missing SEC embeddings\n",
    "        if filing_seq.shape != (sequence_length, embedding_dim):\n",
    "            print(f\"⚠️ Mismatch in SEC embeddings for sequence at index {i}. Using zero vectors.\")\n",
    "            filing_seq = np.zeros((sequence_length, embedding_dim), dtype=np.float32)\n",
    "\n",
    "        combined_seq = np.hstack((stock_seq, filing_seq))  # (seq_len, 7 + embedding_dim)\n",
    "        X.append(combined_seq)\n",
    "        y.append(labels.iloc[i + sequence_length])\n",
    "\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.int64)  # ✅ Convert lists to NumPy first!\n",
    "\n",
    "# ✅ Initialize Model (GRU model is assumed to be already defined)\n",
    "input_size = 7 + embedding_dim  # Stock features (7) + SEC embeddings (Doc2Vec)\n",
    "output_size = 3  # 3 classes: Up, Down, Stable\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ✅ Initialize Model (same architecture as before)\n",
    "model = GRUStockModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "\n",
    "# ✅ Load model weights\n",
    "model.load_state_dict(torch.load(\"stock_gru_d2v.pth\", map_location=device))\n",
    "\n",
    "# ✅ Reinitialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# ✅ Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# ✅ Training Loop (Train Per-File, Save Every 2 Files)\n",
    "train_losses = []\n",
    "file_count = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n🟢 Epoch {epoch+1}/{epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for file in tqdm(train_files, desc=f\"Processing Training Files\"):\n",
    "        if not os.path.isfile(file):  # ✅ Skip missing files\n",
    "            continue\n",
    "\n",
    "        if os.stat(file).st_size == 0:\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(file)\n",
    "        if not {'Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation', '10-K', 'DEF 14A'}.issubset(df.columns):\n",
    "            continue\n",
    "\n",
    "        df = df[['Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation', '10-K', 'DEF 14A']]\n",
    "        labels = create_labels(df)\n",
    "\n",
    "        # ✅ Retrieve SEC Filing Embeddings **Only for This File**\n",
    "        filing_embeddings = np.array([\n",
    "            get_filing_embedding(switch_file_path(str(row[\"10-K\"]))) if row[\"10-K\"] != \"0\"\n",
    "            else get_filing_embedding(switch_file_path(str(row[\"DEF 14A\"]))) if row[\"DEF 14A\"] != \"0\"\n",
    "            else np.zeros((embedding_dim,), dtype=np.float32)\n",
    "            for _, row in df.iterrows()\n",
    "        ])\n",
    "\n",
    "        # ✅ Generate sequences\n",
    "        X, y = create_sequences(df, labels, filing_embeddings, sequence_length)\n",
    "\n",
    "        # ✅ Convert NumPy arrays to tensors **efficiently**\n",
    "        X_train_tensor = torch.from_numpy(X).float().to(device)\n",
    "        y_train_tensor = torch.from_numpy(y).long().to(device)\n",
    "\n",
    "        # ✅ Create DataLoader (Batch Within the File)\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # ✅ Train Per-File (Batch Training within the File)\n",
    "        file_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # If batch_X has nan skip\n",
    "            if torch.isnan(batch_X).any():\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            file_loss += loss.item()\n",
    "\n",
    "        total_loss += file_loss / len(train_loader)\n",
    "        file_count += 1\n",
    "\n",
    "        # ✅ Save Model Every 2 Files\n",
    "        if file_count % save_every_files == 0 or file_count == len(train_files):\n",
    "            torch.save(model.state_dict(), f\"stock_gru_d2v.pth\")\n",
    "\n",
    "    # ✅ Save Model After Each Epoch\n",
    "    torch.save(model.state_dict(), f\"stock_gru_d2v.pth\")\n",
    "    avg_loss = total_loss / len(train_files)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f} ✅ Model Saved!\")\n",
    "\n",
    "# ✅ Save Final Model\n",
    "torch.save(model.state_dict(), \"stock_gru_final.pth\")\n",
    "print(\"\\n✅ Training complete! Final model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1863.14it/s]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1971.31it/s]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2309.81it/s]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1732.74it/s]\n",
      "testing: 100%|██████████| 6137/6137 [00:03<00:00, 1959.87it/s]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1985.63it/s]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1980.62it/s]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1958.65it/s]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 2074.80it/s]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1933.39it/s]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1958.72it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 2006.39it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1947.72it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1943.73it/s]]\n",
      "testing: 100%|██████████| 3687/3687 [00:01<00:00, 1919.24it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1849.48it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1883.14it/s]]\n",
      "testing: 100%|██████████| 5338/5338 [00:02<00:00, 2298.96it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 2048.63it/s]]\n",
      "testing: 100%|██████████| 5122/5122 [00:02<00:00, 2076.01it/s]]\n",
      "testing: 100%|██████████| 3486/3486 [00:01<00:00, 2418.25it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2402.19it/s]]\n",
      "testing: 100%|██████████| 5657/5657 [00:02<00:00, 2230.73it/s]]\n",
      "testing: 100%|██████████| 3739/3739 [00:01<00:00, 2321.18it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2351.41it/s]]\n",
      "testing: 100%|██████████| 4380/4380 [00:02<00:00, 1843.08it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2309.06it/s]]\n",
      "testing: 100%|██████████| 5824/5824 [00:02<00:00, 2246.72it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1918.24it/s]]\n",
      "testing: 100%|██████████| 4302/4302 [00:01<00:00, 2241.39it/s]]\n",
      "testing: 100%|██████████| 3317/3317 [00:01<00:00, 2175.98it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2181.61it/s]]\n",
      "testing: 100%|██████████| 3193/3193 [00:01<00:00, 2229.57it/s]]\n",
      "testing: 100%|██████████| 5913/5913 [00:02<00:00, 2005.24it/s]]\n",
      "testing: 100%|██████████| 1984/1984 [00:00<00:00, 2302.89it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2346.28it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2196.79it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 2068.63it/s]]\n",
      "testing: 100%|██████████| 3460/3460 [00:01<00:00, 2240.08it/s]]\n",
      "testing: 100%|██████████| 5167/5167 [00:02<00:00, 2237.78it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1863.97it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2163.91it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2227.06it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2314.24it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1947.05it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1993.79it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2297.52it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2227.35it/s]]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 2020.65it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2275.45it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1970.52it/s]it]\n",
      "testing: 100%|██████████| 2308/2308 [00:00<00:00, 2330.68it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2317.86it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 2049.01it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 2024.06it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2343.33it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2304.74it/s]it]\n",
      "testing: 100%|██████████| 2579/2579 [00:01<00:00, 2015.70it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1903.36it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1831.78it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2196.93it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1990.22it/s]it]\n",
      "testing: 100%|██████████| 5064/5064 [00:02<00:00, 1899.73it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1902.11it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 2011.54it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1926.36it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1827.71it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1935.94it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1899.49it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 2042.99it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1931.08it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1936.36it/s]it]\n",
      "testing: 100%|██████████| 3292/3292 [00:01<00:00, 1896.84it/s]it]\n",
      "testing: 100%|██████████| 5892/5892 [00:02<00:00, 2134.36it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2100.05it/s]it]\n",
      "testing: 100%|██████████| 3201/3201 [00:01<00:00, 1952.15it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2203.52it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1970.21it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1963.07it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 2039.29it/s]it]\n",
      "testing: 100%|██████████| 186/186 [00:00<00:00, 1829.45it/s]s/it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1934.76it/s]]  \n",
      "testing: 100%|██████████| 4317/4317 [00:02<00:00, 1938.71it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1901.08it/s]it]\n",
      "testing: 100%|██████████| 3065/3065 [00:01<00:00, 1994.94it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1994.78it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1915.97it/s]it]\n",
      "testing: 100%|██████████| 1455/1455 [00:00<00:00, 1851.70it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1953.49it/s]]  \n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1897.87it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1981.54it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1994.46it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1979.59it/s]]  \n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 2017.44it/s]it]\n",
      "testing: 100%|██████████| 5575/5575 [00:04<00:00, 1321.06it/s]it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1854.50it/s]it]\n",
      "testing: 100%|██████████| 5793/5793 [00:02<00:00, 2024.52it/s]/it]\n",
      "testing: 100%|██████████| 6161/6161 [00:03<00:00, 1994.55it/s]/it]\n",
      "testing: 100%|██████████| 6127/6127 [00:03<00:00, 1966.36it/s]/it]\n",
      "testing: 100%|██████████| 5546/5546 [00:03<00:00, 1846.59it/s]/it]\n",
      "testing: 100%|██████████| 2046/2046 [00:00<00:00, 2104.42it/s]/it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1933.92it/s]/it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1766.18it/s]/it]\n",
      "testing: 100%|██████████| 4792/4792 [00:02<00:00, 1798.66it/s]/it]\n",
      "testing: 100%|██████████| 2924/2924 [00:01<00:00, 1897.23it/s]/it]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2179.37it/s]t]  \n",
      "testing: 100%|██████████| 4410/4410 [00:02<00:00, 1836.47it/s]/it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1687.62it/s]/it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 2012.76it/s]/it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1951.78it/s]/it]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1600.58it/s]t]  \n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 2010.12it/s]t]\n",
      "testing: 100%|██████████| 4872/4872 [00:02<00:00, 1905.74it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1968.29it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 2061.46it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 2068.11it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2202.34it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2190.90it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2301.43it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2180.77it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1889.47it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1947.71it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1919.99it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 1745.25it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 2080.20it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2181.13it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2161.02it/s]t]\n",
      "testing: 100%|██████████| 4674/4674 [00:02<00:00, 2099.10it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2249.48it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2186.38it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2130.02it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2162.26it/s]t]\n",
      "testing: 100%|██████████| 2615/2615 [00:01<00:00, 1938.14it/s]t]\n",
      "testing: 100%|██████████| 5543/5543 [00:02<00:00, 1958.93it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 2027.09it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2211.48it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:03<00:00, 2000.38it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2188.02it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2208.79it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2271.61it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2181.96it/s]t]\n",
      "testing: 100%|██████████| 2896/2896 [00:01<00:00, 2187.75it/s]t]\n",
      "testing: 100%|██████████| 2382/2382 [00:01<00:00, 2231.87it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2199.33it/s]t]\n",
      "testing: 100%|██████████| 5917/5917 [00:02<00:00, 2177.35it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2336.98it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2110.39it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2200.41it/s]t]\n",
      "testing: 100%|██████████| 6281/6281 [00:02<00:00, 2185.87it/s]t]\n",
      "stock files: 100%|██████████| 150/150 [3:14:28<00:00, 77.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 **Overall Test Set Performance Metrics** 🔹\n",
      "✅ Accuracy: 0.7721\n",
      "📏 Precision: 0.5961\n",
      "📡 Recall: 0.7721\n",
      "⚖️ F1-Score: 0.6728\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def create_labels(df):\n",
    "    \"\"\"Create classification labels for stock price movement.\"\"\"\n",
    "    df['pct change'] = df['Close'].pct_change(N)\n",
    "    df['Close_diff_pct'] = df['pct change'].shift(-N)\n",
    "    df['Label'] = 0  \n",
    "    df.loc[df['Close_diff_pct'] > threshold, 'Label'] = 1  \n",
    "    df.loc[df['Close_diff_pct'] < -threshold, 'Label'] = 2  \n",
    "    return df['Label']\n",
    "\n",
    "def get_filing_embedding(file_path):\n",
    "    \"\"\"Retrieves embedding for a single SEC filing, ensuring shape consistency.\"\"\"\n",
    "    embedding = np.zeros((embedding_dim,), dtype=np.float32)  # Default zero vector\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            embedding = doc2vec_model.infer_vector(text.split())\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error retrieving embedding for {file_path}: {e}\")\n",
    "\n",
    "    # ✅ Ensure embedding has correct shape\n",
    "    if embedding.shape != (embedding_dim,):\n",
    "        print(f\"⚠️ Bad embedding shape: {embedding.shape} for {file_path}. Using zero vector.\")\n",
    "        embedding = np.zeros((embedding_dim,), dtype=np.float32)\n",
    "\n",
    "    return embedding\n",
    "\n",
    "def create_sequences(df, labels, filing_embeddings, sequence_length):\n",
    "    \"\"\"Create sequences for GRU training including SEC filing embeddings.\"\"\"\n",
    "    stock_features = df[['Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation']].values\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(len(stock_features) - sequence_length - N):\n",
    "        stock_seq = stock_features[i:i + sequence_length]  # (seq_len, 7)\n",
    "        filing_seq = filing_embeddings[i:i + sequence_length]  # (seq_len, embedding_dim)\n",
    "\n",
    "        # ✅ Fix potential missing SEC embeddings\n",
    "        if filing_seq.shape != (sequence_length, embedding_dim):\n",
    "            print(f\"⚠️ Mismatch in SEC embeddings for sequence at index {i}. Using zero vectors.\")\n",
    "            filing_seq = np.zeros((sequence_length, embedding_dim), dtype=np.float32)\n",
    "\n",
    "        combined_seq = np.hstack((stock_seq, filing_seq))  # (seq_len, 7 + embedding_dim)\n",
    "        X.append(combined_seq)\n",
    "        y.append(labels.iloc[i + sequence_length])\n",
    "\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.int64)  # ✅ Convert lists to NumPy first!\n",
    "\n",
    "sequence_length = 7  # Lookback window\n",
    "N = 1\n",
    "threshold = 0.02  # 2% change threshold for \"Stable\"\n",
    "\n",
    "# ✅ Load Pretrained Doc2Vec Model\n",
    "doc2vec_model = Doc2Vec.load(\"sec_doc2vec.model\")\n",
    "embedding_dim = doc2vec_model.vector_size  # Get embedding size from Doc2Vec\n",
    "\n",
    "# ✅ Define Parent Folder for Stock Data\n",
    "PARENT_FOLDER = \"/Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/stock-data/\"\n",
    "\n",
    "# ✅ Get list of test files\n",
    "csv_files = [os.path.join(PARENT_FOLDER, file) for file in os.listdir(PARENT_FOLDER) if file.endswith(\".csv\")]\n",
    "test_files = csv_files[int(0.7 * len(csv_files)):]\n",
    "\n",
    "# ✅ Load GRU Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GRUStockModel(input_size=7+embedding_dim, hidden_size=128, num_layers=2, output_size=3).to(device)\n",
    "model.load_state_dict(torch.load(\"stock_gru_d2v_final.pth\"))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# ✅ Initialize Metrics Storage\n",
    "all_actuals, all_predictions = [], []\n",
    "\n",
    "# ✅ Process Each Test Stock\n",
    "for test_stock in tqdm(test_files, desc=\"stock files\"):\n",
    "    # If file is empty skip\n",
    "    if os.stat(test_stock).st_size == 0:\n",
    "        continue\n",
    "\n",
    "    df_test = pd.read_csv(test_stock)\n",
    "    if not {'Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation', '10-K', 'DEF 14A'}.issubset(df_test.columns):\n",
    "        continue  # Skip if missing columns\n",
    "\n",
    "    # Select Relevant Features\n",
    "    df_test = df_test[['Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation', '10-K', 'DEF 14A']]\n",
    "\n",
    "    labels = create_labels(df_test)\n",
    "\n",
    "    # ✅ Retrieve SEC Filing Embeddings **Only for This File**\n",
    "    filing_embeddings = np.array([\n",
    "        get_filing_embedding(switch_file_path(str(row[\"10-K\"]))) if row[\"10-K\"] != \"0\"\n",
    "        else get_filing_embedding(switch_file_path(str(row[\"DEF 14A\"]))) if row[\"DEF 14A\"] != \"0\"\n",
    "        else np.zeros((embedding_dim,), dtype=np.float32)\n",
    "        for _, row in df_test.iterrows()\n",
    "    ])\n",
    "\n",
    "    # ✅ Generate sequences\n",
    "    X, y = create_sequences(df_test, labels, filing_embeddings, sequence_length)\n",
    "\n",
    "    # ✅ Convert NumPy arrays to tensors **efficiently**\n",
    "    X_test = torch.from_numpy(X).float().to(device)\n",
    "    y_test = torch.from_numpy(y).long().to(device)\n",
    "\n",
    "    # ✅ Run Inference\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(X_test)), desc=\"testing\"):\n",
    "            X_sample = X_test[i].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "            if torch.isnan(X_sample).any():\n",
    "                continue\n",
    "\n",
    "            y_actual = y_test[i].item()\n",
    "            output = model(X_sample)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            # Store results\n",
    "            all_actuals.append(y_actual)\n",
    "            all_predictions.append(predicted.item())\n",
    "\n",
    "# ✅ Compute Classification Metrics\n",
    "accuracy = accuracy_score(all_actuals, all_predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_actuals, all_predictions, average=\"weighted\", zero_division=0)\n",
    "\n",
    "# ✅ Print Results\n",
    "print(\"\\n🔹 **Overall Test Set Performance Metrics** 🔹\")\n",
    "print(f\"✅ Accuracy: {accuracy:.4f}\")\n",
    "print(f\"📏 Precision: {precision:.4f}\")\n",
    "print(f\"📡 Recall: {recall:.4f}\")\n",
    "print(f\"⚖️ F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting SEC Filings: 100%|██████████| 497/497 [00:06<00:00, 76.45it/s]\n",
      "switch: 100%|██████████| 5561498/5561498 [00:00<00:00, 7983377.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 19779 SEC filings for training.\n",
      "✅ Using 1000 SEC filings for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved progress after batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved progress after batch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved progress after batch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved progress after batch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved progress after batch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved progress after batch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved progress after batch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved progress after batch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved progress after batch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch proc: 100%|██████████| 10/10 [43:15<00:00, 259.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved progress after batch 10\n",
      "✅ Training complete! Final model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# ✅ Paths\n",
    "stock_folder = \"/Users/colbywang/Google Drive/我的云端硬盘/Advanced NLP/Assignments/data files/organized/stock-data\"\n",
    "all_stock_files = [os.path.join(stock_folder, f) for f in os.listdir(stock_folder) if f.endswith(\".csv\")]\n",
    "model_save_path = \"sec_doc2vec.model\"\n",
    "\n",
    "batch_size = 100  # Number of filings to process at once\n",
    "\n",
    "# ✅ Extract SEC Filing Paths from Stock CSVs\n",
    "sec_files = []\n",
    "for csv_file in tqdm(all_stock_files, desc=\"Extracting SEC Filings\"):\n",
    "    if os.stat(csv_file).st_size == 0:\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    if not {'Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation'}.issubset(df.columns):\n",
    "        continue  # Skip if missing columns\n",
    "    \n",
    "    if \"10-K\" in df.columns and \"DEF 14A\" in df.columns:\n",
    "        # Must be string and not 0\n",
    "        sec_files.extend(df[\"10-K\"].astype(str).tolist())\n",
    "        sec_files.extend(df[\"DEF 14A\"].astype(str).tolist())\n",
    "\n",
    "# ✅ Convert Paths (add tqdm)\n",
    "sec_files = [switch_file_path(f) for f in tqdm(sec_files, desc=\"switch\") if f != \"0\"]\n",
    "\n",
    "print(f\"✅ Found {len(sec_files)} SEC filings for training.\")\n",
    "\n",
    "# ✅ Shuffle the file list\n",
    "random.shuffle(sec_files)\n",
    "\n",
    "# Get 900 of the files for training\n",
    "sec_files = sec_files[:1000]\n",
    "print(f\"✅ Using {len(sec_files)} SEC filings for training.\")\n",
    "\n",
    "# ✅ Load SEC filings into TaggedDocument format\n",
    "def load_filing(file_path):\n",
    "    \"\"\"Loads and tokenizes an SEC filing.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return text.split()  # Tokenized as words\n",
    "\n",
    "# ✅ Initialize Model (Adjust parameters as needed)\n",
    "model = Doc2Vec(\n",
    "    vector_size=384,\n",
    "    window=10,\n",
    "    min_count=5,\n",
    "    workers=max(1, os.cpu_count() - 2),  # Prevent CPU overload\n",
    "    epochs=20,\n",
    "    dm=1  # PV-DM mode\n",
    ")\n",
    "\n",
    "# ✅ Process in Batches and Save After Each Batch\n",
    "for i in tqdm(range(0, len(sec_files), batch_size), desc=\"batch proc\"):\n",
    "    batch_docs = [\n",
    "        TaggedDocument(words=load_filing(file), tags=[str(j)])\n",
    "        for j, file in enumerate(sec_files[i:i + batch_size])\n",
    "    ]\n",
    "\n",
    "    # ✅ If first batch, build vocab\n",
    "    if i == 0:\n",
    "        model.build_vocab(batch_docs)\n",
    "    else:\n",
    "        model.build_vocab(batch_docs, update=True)  # Update existing vocab\n",
    "\n",
    "    # ✅ Train on batch\n",
    "    model.train(batch_docs, total_examples=len(batch_docs), epochs=model.epochs)\n",
    "\n",
    "    # ✅ Save after each batch\n",
    "    model.save(model_save_path)\n",
    "    print(f\"✅ Saved progress after batch {i // batch_size + 1}\")\n",
    "\n",
    "print(\"✅ Training complete! Final model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
