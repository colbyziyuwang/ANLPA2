{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# GRU Model Definition\n",
    "class GRUStockModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRUStockModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # Take last time step output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to update parent_folder and colab_base as you see fit\n",
    "\n",
    "Download Ollama first to access llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global paths for both local (Mac) and Google Colab\n",
    "PARENT_FOLDER = \"/Users/colbywang/Google Drive/ÊàëÁöÑ‰∫ëÁ´ØÁ°¨Áõò/Advanced NLP/Assignments/data files/organized/\"\n",
    "COLAB_BASE = \"/content/gdrive/MyDrive/Assignments/Advanced NLP/Assignments/data files/organized/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# ‚úÖ Initialize Embedding Model (Same as Before)\n",
    "embedding_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "Settings.embed_model = embedding_model  # Apply globally\n",
    "\n",
    "# ‚úÖ Initialize Sentence Window Parser (Used for Retrieval)\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=1,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\"\n",
    ")\n",
    "\n",
    "def build_vector_index(file_path):\n",
    "    \"\"\"\n",
    "    Build a vector index from a given document.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the document.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: Built vector store index.\n",
    "    \"\"\"\n",
    "    # ‚úÖ Load document\n",
    "    docs = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
    "\n",
    "    # ‚úÖ Process nodes from document\n",
    "    nodes = node_parser.get_nodes_from_documents(docs)\n",
    "\n",
    "    # ‚úÖ Create Vector Store Index\n",
    "    index = VectorStoreIndex(nodes)\n",
    "\n",
    "    return index\n",
    "\n",
    "def retrieve_avg_embedding(query_text, vector_index, top_k=3):\n",
    "    \"\"\"\n",
    "    Retrieve top-K relevant passages and compute their average embedding.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The query to search in the vector database.\n",
    "        vector_index (VectorStoreIndex): Prebuilt vector store index.\n",
    "        top_k (int): Number of retrieved passages to consider.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The averaged embedding of retrieved contexts.\n",
    "    \"\"\"\n",
    "    # Retrieve top-K most similar documents\n",
    "    retriever = vector_index.as_retriever(similarity_top_k=top_k)\n",
    "    retrieved_docs = retriever.retrieve(query_text)\n",
    "\n",
    "    # Extract embeddings of retrieved docs\n",
    "    retrieved_embeddings = np.array([\n",
    "        doc.embedding for doc in retrieved_docs if doc.embedding is not None\n",
    "    ])\n",
    "\n",
    "    # Handle case where no embeddings were retrieved\n",
    "    if retrieved_embeddings.size == 0:\n",
    "        print(\"‚ö†Ô∏è No valid embeddings found. Returning zero vector.\")\n",
    "        return np.zeros((embedding_model.get_embedding_dimension(),))\n",
    "\n",
    "    # Compute the average embedding\n",
    "    avg_embedding = np.mean(retrieved_embeddings, axis=0)\n",
    "\n",
    "    return avg_embedding # shape: (384, )\n",
    "\n",
    "# ‚úÖ Example Usage\n",
    "# file_path = \"path/to/your/10-K_or_DEF14A_file.txt\"\n",
    "# query_text = \"Summarize key financial risks.\"\n",
    "\n",
    "# # Build index\n",
    "# vector_index = build_vector_index(file_path)\n",
    "\n",
    "# # Retrieve average embedding\n",
    "# avg_embedding = retrieve_avg_embedding(query_text, vector_index)\n",
    "# print(\"Avg Embedding Shape:\", avg_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_file_path(colab_path):\n",
    "    \"\"\"\n",
    "    Converts a file path from Google Drive (Colab) to a local path.\n",
    "\n",
    "    Args:\n",
    "        colab_path (str): The file path from Google Drive in Colab.\n",
    "\n",
    "    Returns:\n",
    "        str: The equivalent local path.\n",
    "    \"\"\"\n",
    "    local_path = colab_path.replace(COLAB_BASE, PARENT_FOLDER, 1)\n",
    "    return local_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 347 stock files, Testing on 150 stock files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading training files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 347/347 [02:26<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [06:49<00:00, 148.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.6834\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [06:30<00:00, 155.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Loss: 0.6833\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [06:48<00:00, 148.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Loss: 0.6832\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [07:04<00:00, 143.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Loss: 0.6832\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [06:21<00:00, 159.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: 0.6833\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [06:26<00:00, 157.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Loss: 0.6833\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [06:38<00:00, 152.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Loss: 0.6833\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [06:29<00:00, 156.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Loss: 0.6833\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [06:17<00:00, 160.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Loss: 0.6833\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [06:16<00:00, 161.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Loss: 0.6833\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [06:19<00:00, 160.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Loss: 0.6833\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [06:40<00:00, 151.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Loss: 0.6833\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [06:53<00:00, 147.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Loss: 0.6833\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [07:40<00:00, 132.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Loss: 0.6833\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [06:45<00:00, 150.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Loss: 0.6833\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [07:18<00:00, 138.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Loss: 0.6833\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [05:47<00:00, 175.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Loss: 0.6833\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [07:09<00:00, 141.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Loss: 0.6833\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [06:55<00:00, 146.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Loss: 0.6833\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60831/60831 [06:49<00:00, 148.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Loss: 0.6833\n",
      "\n",
      "‚úÖ Training complete. Training loss visualization saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to stock data folder\n",
    "PARENT_FOLDER = \"/Users/colbywang/Google Drive/ÊàëÁöÑ‰∫ëÁ´ØÁ°¨Áõò/Advanced NLP/Assignments/data files/organized/stock-data\"\n",
    "all_files = glob(os.path.join(PARENT_FOLDER, \"*.csv\"))\n",
    "\n",
    "# Hyperparameters\n",
    "sequence_length = 7  # Lookback window\n",
    "N = 1  # Predict trend in next N days\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "threshold = 0.02  # 2% change threshold for \"Stable\"\n",
    "train_ratio = 0.7  # 70% of files for training, 30% for testing\n",
    "\n",
    "# Split stock files into training and testing sets\n",
    "train_files, test_files = train_test_split(all_files, test_size=1-train_ratio, random_state=42)\n",
    "print(f\"Training on {len(train_files)} stock files, Testing on {len(test_files)} stock files\")\n",
    "\n",
    "def create_labels(df):\n",
    "    \"\"\"\n",
    "    Create classification labels for stock price movement.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Stock data\n",
    "        N (int): Number of days in the future to compute price movement\n",
    "        threshold (float): Percentage change threshold to classify Up or Down\n",
    "\n",
    "    Returns:\n",
    "        Series: Labels (0 = Stable, 1 = Up, 2 = Down)\n",
    "    \"\"\"\n",
    "    df['pct change'] = df['Close'].pct_change(N)\n",
    "    df['Close_diff_pct'] = df['pct change'].shift(-N)\n",
    "\n",
    "    # Default to \"Stable\"\n",
    "    df['Label'] = 0  \n",
    "\n",
    "    # Up (if change > threshold)\n",
    "    df.loc[df['Close_diff_pct'] > threshold, 'Label'] = 1  \n",
    "\n",
    "    # Down (if change < -threshold)\n",
    "    df.loc[df['Close_diff_pct'] < -threshold, 'Label'] = 2  \n",
    "\n",
    "    return df['Label']\n",
    "\n",
    "def create_sequences(df, labels, sequence_length, N=5):\n",
    "    \"\"\"\n",
    "    Create sequences for GRU training.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Stock features\n",
    "        labels (Series): Classification labels\n",
    "        sequence_length (int): Length of input sequence\n",
    "        N (int): Lookahead period for labels\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: X (features)\n",
    "        np.ndarray: y (labels)\n",
    "    \"\"\"\n",
    "    df = df[['Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation']]\n",
    "    X, y = [], []\n",
    "\n",
    "    # Ensure labels align with future price movement\n",
    "    for i in range(len(df) - sequence_length - N):\n",
    "        X.append(df.values[i:i + sequence_length])  # Sequence of input features\n",
    "        y.append(labels.iloc[i + sequence_length])  # Label for the next movement\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load and preprocess all training stock data\n",
    "all_X_train, all_y_train = [], []\n",
    "\n",
    "for file in tqdm(train_files, desc=\"Loading training files\"):\n",
    "    # If file is empty skip\n",
    "    if os.stat(file).st_size == 0:\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(file)\n",
    "    if not {'Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation'}.issubset(df.columns):\n",
    "        continue  # Skip if missing columns\n",
    "\n",
    "    df = df[['Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation']]\n",
    "\n",
    "    labels = create_labels(df)\n",
    "    X, y = create_sequences(df, labels, sequence_length)\n",
    "\n",
    "    all_X_train.append(X)\n",
    "    all_y_train.append(y)\n",
    "\n",
    "# Concatenate all training sequences\n",
    "X_train = np.concatenate(all_X_train, axis=0)\n",
    "y_train = np.concatenate(all_y_train, axis=0)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "# Create PyTorch DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize Model\n",
    "input_size = 7  # ['Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation']\n",
    "output_size = 3  # 3 classes: Up, Down, Stable\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GRUStockModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train Model\n",
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        # If batch_X has nan skip\n",
    "        if torch.isnan(batch_X).any():\n",
    "            continue\n",
    "        \n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save Training Loss Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"train_loss.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n‚úÖ Training complete. Training loss visualization saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved as 'stock_model.pth'.\n"
     ]
    }
   ],
   "source": [
    "# Save Model\n",
    "torch.save(model.state_dict(), \"stock_gru_model.pth\")\n",
    "print(\"‚úÖ Model saved as 'stock_model.pth'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stock files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [06:29<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ **Overall Test Set Performance Metrics** üîπ\n",
      "‚úÖ Accuracy: 0.7716\n",
      "üìè Precision: 0.6080\n",
      "üì° Recall: 0.7716\n",
      "‚öñÔ∏è F1-Score: 0.6726\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# ‚úÖ Define Parent Folder for Stock Data\n",
    "PARENT_FOLDER = \"/Users/colbywang/Google Drive/ÊàëÁöÑ‰∫ëÁ´ØÁ°¨Áõò/Advanced NLP/Assignments/data files/organized/stock-data/\"\n",
    "\n",
    "# ‚úÖ Get list of test files\n",
    "csv_files = [os.path.join(PARENT_FOLDER, file) for file in os.listdir(PARENT_FOLDER) if file.endswith(\".csv\")]\n",
    "test_files = csv_files[int(0.7 * len(csv_files)):]\n",
    "\n",
    "# ‚úÖ Load GRU Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GRUStockModel(input_size=7, hidden_size=64, num_layers=2, output_size=3).to(device)\n",
    "model.load_state_dict(torch.load(\"stock_gru_model.pth\"))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# ‚úÖ Initialize Metrics Storage\n",
    "all_actuals, all_predictions = [], []\n",
    "\n",
    "# ‚úÖ Process Each Test Stock\n",
    "for test_stock in tqdm(test_files, desc=\"stock files\"):\n",
    "    # If file is empty skip\n",
    "    if os.stat(test_stock).st_size == 0:\n",
    "        continue\n",
    "\n",
    "    df_test = pd.read_csv(test_stock)\n",
    "    if not {'Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation'}.issubset(df.columns):\n",
    "        continue  # Skip if missing columns\n",
    "\n",
    "    # Select Relevant Features\n",
    "    df_test = df_test[['Close', 'High', 'Low', 'Open', 'Volume', 'CPI', 'Inflation']]\n",
    "\n",
    "    # Generate Labels\n",
    "    labels_test = create_labels(df_test)\n",
    "\n",
    "    # Create Sequences\n",
    "    X_test, y_test = create_sequences(df_test, labels_test, sequence_length=7)\n",
    "    X_test, y_test = torch.tensor(X_test, dtype=torch.float32).to(device), torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    # ‚úÖ Run Inference\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X_test)):\n",
    "            X_sample = X_test[i].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "            if torch.isnan(X_sample).any():\n",
    "                continue\n",
    "\n",
    "            y_actual = y_test[i].item()\n",
    "            output = model(X_sample)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            # Store results\n",
    "            all_actuals.append(y_actual)\n",
    "            all_predictions.append(predicted.item())\n",
    "\n",
    "# ‚úÖ Compute Classification Metrics\n",
    "accuracy = accuracy_score(all_actuals, all_predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_actuals, all_predictions, average=\"weighted\", zero_division=0)\n",
    "\n",
    "# ‚úÖ Print Results\n",
    "print(\"\\nüîπ **Overall Test Set Performance Metrics** üîπ\")\n",
    "print(f\"‚úÖ Accuracy: {accuracy:.4f}\")\n",
    "print(f\"üìè Precision: {precision:.4f}\")\n",
    "print(f\"üì° Recall: {recall:.4f}\")\n",
    "print(f\"‚öñÔ∏è F1-Score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
